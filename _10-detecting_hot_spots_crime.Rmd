# Chapter 10: Assessing local clusters and repeats

## Introduction

There has been a great interest within criminology and crime analysis in the study of local clusters of crime or hot spots over the last 30 years. Hot spots is the term typically used within criminology and crime analysis to refer to small geographical areas with a high concentration of crime. @Weisburd_2015 argues for a *law of concentration* of crime that postulates that a large proportion of crime events occur at relatively few places (such as specific addresses, street intersections, street blocks, or other small micro places) within larger geographical areas such as cities. In one of the earlier contributions to this field of research, @Sherman_1989 noted how roughly 3% of all addresses in Minneapolis (USA) generated about 50% of all calls to police services. A number of subsequent studies have confirmed this pattern elsewhere. @Steenbeek_2016 argue that, in fact, most of the geographical variability of crime (58% to 69% in their data from The Hague) can be attributed to micro geographic units, with a very limited contribution from the neighbourhood level. This literature also argues that many crime hot spots are relatively stable over time (@Andresen_2011b; @Andresen_2017; @Andresen_2017b).

There is also now a considerable body of research evaluating police interventions that take this insight as key for articulating responses to crime. **Hot spots policing**, or place based policing, assumes that police can reduce crime by focusing their resources on the small number of places that generate a majority of crime problems (@Braga_2010). Policing crime hot spots has become a popular strategy in the US and efforts to adopt this approach have also taken place elsewhere. Theoretically one could use all sort of proactive creative approaches to solve crime in these locations (using for example conceptual and tactical tools from @Goldstein_1990 problem oriented policing approach), though too often in practice directed patrol becomes the default response. Recent reviews of the literature (dominated by the US experience) suggest the approach can have a small effect size (@Braga_2019), though some authors contend alternative measures of effect size (which are suggestive of a moderate effect) should be used instead (@Braga_2020). Whether these findings would replicate well in other contexts is still an open question (see, for example, @Collazos_2020)

In the previous chapter we explored ways to visualise the concentration of crime incidents in particular location as a way to explore places with an elevated intensity of crime. We used tesselation and kernel density estimation as a way to do this. As noted by @Chainey_2020:

>"Hot spot maps generated using KDE are useful for showing where crime concentrates but may fail to determine unambiguosly what is hot (in hot spot analysis terms) from what is not hot. That is, a KDE hot spot map may fail to separate the significant spatial concentrations of crime from those spatial distributions of less interest, or from random variation"

There are a number of tools that have been developed to detect local clusters among random variation. In crime analysis, for example, is common the use of the local $Gi^*$ statistic, which is a type of a **local indicator of spatial autocorrelation**. Another popular LISA is the local Moran´s $I$, developed by @Anselin_1995. 

These LISA statistics serve two purposes. On one hand, they may be interpreted as indicators of local pockets of nonstationarity, or hot spots. On the other hand, they may be used to assess the influence of individual locations on the magnitude of the global statistic and to identify “outliers” (@Anselin_1995). @Anselin_1995 suggests that any LISA satisfies two requirements (p. 94:
"-a) the LISA for each observation gives an indication of the extent of significant spatial clustering of similar values around that observation;
- b) the sum of LISAs for all observations is proportional to a global indicator of spatial association".

These measures of local spatial dependence are available in tools often used by crime analysts (CrimeStat, GeoDa, ArcGIS, and also R). But it is important to realise that there are some subtle differences in implementation that reflect design decisions by the programmers. These differences in design may result on apparent differences in numerical findings generated with these different programs. The defaults used by different software may give the impression different results are achieved. @Bivand_2018 offers an excellent thorough review of these different implementations (and steps needed for ensuring comparability). Our focus here will be in introducing the functionality of the `spdep` packages, with a cursory overview of `rgeoda` as well.

Aside from these LISAS, in spatial epidemiology a number of techniques have been developed for detecting areas with an unusually high risk of disease. Although originally developed in the context of studying disease, the techniques can also be applied to the study of crime. Most of the methods in spatial epidemiology are based on the idea of "moving windows", such as the spatial scan statistic developed by @Kulldorff_1997. Several packages bring this scan statistics to R. There are also other packages that use different algorithms to detect disease clusters, such as AMOEBA, that is based in the Getis-Ord algorithm. As we will see we are spoiled for choice in this regard. Here we will only provide a glimpse into how to perform a few of these tests with R.

[[[In crime analysis is also relevant the study of near repeat victimisation. bla bla bla...]]]

For this section we will need the following packages:

```{r, message=FALSE, warning=FALSE}

library(dplyr)
# Packages for handling spatial data and for geospatial carpentry
library(sf)
library(raster)
library(spdep)
# Packages for detecting clusters
library(DCluster)

library(SpatialEpi)
library(smerc)
library(surveillance)
library(DClusterm)

# Packages for detecting near repeat victimisation
## You will need to install the following package from github
## You can use the following code for it: 
## remotes::install_github("wsteenbeek/NearRepeat", build_vignettes = TRUE)
library(NearRepeat)

# Packages for visualisation and mapping
library(tmap)
library(mapview)
library(RColorBrewer)
```

## Burglaries in Manchester

We return to burglaries in Manchester for this chapter. The files we are loading include the geomasked locations of burglary occurrences in 2017, for the whole of Manchester city and separately for its city centre. Later in this chapter we will look at the whole of Manchester and we will use counts of burglary per LSOA, so we will also load this aggregated data now.

```{r, message=FALSE, warning=FALSE}

# Create sf object with burglaries for Manchester city
manc_burglary <- st_read("data/manc_burglary_bng.geojson",
                         quiet=TRUE)
manc_burglary <- st_transform(manc_burglary , 4326)
# Create sf object with burglaries in city centre
cc_burglary <- st_read("data/manch_cc_burglary_bng.geojson",
                       quiet=TRUE)
cc_burglary <- st_transform(cc_burglary , 4326)
# Read the boundary for the Manchester city centre ward
city_centre <- st_read("data/manchester_wards.geojson", 
                       quiet = TRUE) %>%
  filter(wd16nm == "City Centre")
city_centre <- st_transform(city_centre, 4326)
# Read the boundary for Manchester city
manchester <- st_read("data/manchester_city_boundary_bng.geojson",
                      quiet = TRUE)
manchester <- st_transform(manchester , 4326)
# Read into R the count of burglaries per LSOA
burglary_lsoa <- st_read("data/burglary_manchester_lsoa.geojson",
                    quiet = TRUE)
# Place last file in projected British National Grid
burglary_lsoa <- st_transform(burglary_lsoa, 27700)

```


## Micro Grid Cells

[[[]]]

In chapter 4 we discussed alternatives to our choropleth maps, and one of these was to use a grid map. This is a process whereby a tessalating grid of a shape such as squares or hexagons is overlaid on top of our area of interest, and our points are aggregated to such a grid. We explored this using `ggplot2` package. We also used this approach when discussing quadrat counting with `ppp` objects and using the functionality provided by the `spatstat` package.

Here we can revisit this with another approach, using the `raster` package. First we need to create the grid for the raster using the `raster()` function. The first argument defines the extent for the grid. By using our "toronto" `sf` object we ensure it includes all of it. Then the `resolution` argument simply defines how large we want the cells in this grid to be. You can play around with this argument and see how the results will vary. Then we will use the `rasterize()` function to count the number of points that fall within each cell. This is just a form of quadrant counting as the one we introduced in Chapter 6. The `rasterize()` function takes as the first argument the coordinates of the points. The second argument provides the grid we just created. The `field` argument in this case is assigned a value of `1`  so that each point is counted a unit. Then we use the `fun` argument to specify we want to `"count"` the points within each cell. The `rasterize()` function by default will set at NA all the values where there are no points. We don´t want this to be the case. So that the raster include those cells as zero we use the `background` argument and set it to zero: 

```{r}
# Create empty raster grid
manchester_r <- raster(manchester, resolution = 0.004)
# Fill the grid with the count of incidents within each cell
m_burglary_raster <- rasterize(manc_burglary,
                                manchester_r,
                                field = 1,
                                fun = "count",
                                background = 0)
# Use base R functionality to plot the raster
plot(m_burglary_raster)
plot(st_geometry(manchester), border='#00000040', add=T)
```

This kind of map is helpful if you want to get a sense of the distribution over the whole city, but oftenwise from a crime analyst operational point of view the interest is at a larger scale of resolution. We want to deep in and observe the micro-places that may be attracting crime within certain parts of the city. We can do this by focusing on particular neighbourhoods.

Let´s look at Kthe city centre, the ward with one of the highest counts of burglaries.

```{r, warning = FALSE, message = FALSE}
# Create empty grid for the City Centre
city_centre_r <- raster(city_centre, resolution = 0.0009)
# Produce raster object with the number of incidents within each cell
burglary_raster_cc <- rasterize(cc_burglary,
                                city_centre_r,
                                field = 1,
                                fun = "count",
                                background = 0)
# Plot results with base R
plot(burglary_raster_cc)
plot(st_geometry(city_centre), border='#00000040', add=T)
```

We don´t have in this raster object incidents outside the boundaries of the neighborhoud, so we could prefer to declare those cells as NAs. We can do that with the `raster::mask()` function.

```{r}
burglary_raster_cc <- mask(burglary_raster_cc, city_centre)
plot(burglary_raster_cc)
```

Let´s add some context. We will use `mapview` for it. This is another great package if you want to give your audience some interactivity. It does what is says in the tin: 
>it "provides functions to very quickly and conveniently create interactive visualisations of spatial data. Its main goal is to fill the gap of quick (not presentation grade) interactive plotting to examine and visually investigate both aspects of spatial data, the geometries and their attributes. It can also be considered a data-driven API for the leaflet package as it will automatically render correct map types, depending on the type of the data (points, lines, polygons, raster)." 

With very few lines of code you can get an interactive plot. Here we first will use `RColorBrewer::brewer.pal()` to define a convenient palette and then use `mapview::mapview()` to plot our raster. Below you see how we increase the transparency with the `alpha.region` argument and how we use `col.regions` to change from the default palette to the one we created.

```{r, message=FALSE, warning=FALSE}

my.palette <- brewer.pal(n = 9, name = "OrRd")
mapview(burglary_raster_cc, 
        alpha.regions = 0.7,
        col.regions = my.palette)
```

## Local Getis-Ord

The maps above and the ones we introduced in chapter 6 produced through kernel density estimation are helpful for visualising the varying intensity of crime across the study region of interest. However, as we noted in Chapter 6, it may be hard to discern random variation from clustering of crime incidents simply using these techniques. There are a number of statistical tools that have been developed to extract the signal from the noise, to determine what is hot from random variation, and that were originally designed to detect clusters of disease.

In crime analysis is common to use a technique developed by Arthur Getis and JK Ord in the early 1990s (@Getis_1992 and @Ord_1995), partly because it was implemented in the widely successful ArcGIS as part of their hot spot analysis tool. This statistic provides a measure of spatial dependence at the local level. They are not testing homogeneity, they are testing dependence of a given attribute around neighbouring areas (which could be operationalised as micro cells in a grid, as above, or could be some form of administrative areas). 

Whereas the tests we covered in the previous chapter measure dependence at the global level (whereas areas are surrounded by alike areas), these measures of local clustering try to identify pockets of dependence. They are also called **local indicators of spatial autocorrelation**. It is indeed useful for us to be able to assess quantitatively whether crime events cluster in a non-random manner. in the words of Jerry @Ratcliffe_2010, however, "while a global Moran’s I test can show that crime events cluster in a non-random manner, this simply explains what most criminal justice students learn in their earliest classes." (p. 13). For a crime analyst and practitioner, whereas clustering is of interest, learning about the existence and location of local clusters is of paramount importance.

There are two variants of the local Getis-Ord statistic. The local $G$ is computed as a ratio of the weighted average of the values of the attribute of interest in the neighboring locations, not including the value at the location. It is generally used for spread and diffusion studies. The local $G^*$,on the other hand, includes the value at the location in both numerator and denominator. It is more generally used for clustering studies. High positive values indicate the possibility of a local cluster of high values of the variable being analysed ("hot spot"), very low relative values a similar cluster of low values ("cold spot"). "A value larger than the mean (or, a positive value for a standardized z-value) suggests a High-High cluster or hot spot, a value smaller than the mean (or, negative for a z-value) indicates a Low-Low cluster or cold spot." (@Anselin_2020a) 

For statistical inference, a Bonferroni-type test is suggested in the the papers by Getis and Ord, where tables of critical values are included. The critical values for the 95th percentile under the assumptions discussed by @Ord_1995 are for $n$=30: 2.93, $n$=50: 3.08, $n$=100: 3.29, $n$=500: 3.71, and for $n$=1000: 3.89. @Anselin_2020a suggest that this analytical approximation may not be reliable in practice and that conditional permutation is advisable. Inference with these local measures are also complicated by problems of multiple comparisons, so it is typically advisable to use some form of adjustment to this problem.

The first step in computing the Getis and Ord statistics involves defining the neighbours of each area. And to do that first we have to turn each cell in our grid into a polygon, so we basically move from the raster to the vector model by virtue of using `raster::rasterToPolygons`. 

```{r}
# Move to vector model
grid_cc <-  rasterToPolygons(burglary_raster_cc)

```

What is a neighbour and the code related to defining neighbours is a topic we covered in the previous chapter. For brevity, we will only use the Queen criteria for this illustration. Only to indicate that `include.self()` ensures we get the $G^*$ variation of the local Getis-Ord test, the second variation we mentioned above.

```{r}
nb_queen_cc <-  poly2nb(grid_cc)
lw_queen_cc <- nb2listw(include.self(nb_queen_cc), zero.policy=T)

```

Now that we have the data ready we can, analytically, compute the statistic using the `localG()` function. We can then add the computed local $Gi^*$ to each cell.

```{r, message=FALSE, warning=FALSe}

# Perform the local G analysis (Getis-Ord GI*)
grid_cc$G_hotspot_z <- as.vector(localG(grid_cc$layer, lw_queen_cc))

```

We can then see the produced z scores:

```{r}
summary(grid_cc$G_hotspot_z)

```

How do you interpret these? They are z scores. So you expect large absolute values to index the existence of cold spots (if they are negative) or hot spots (if they are positive). Our sample size is 475 cells and there are only a very small handful of cells with values that reach the critical value in our dataset.

A known problem with this statistic is that of multiple comparisons. As noted by @Pebesma_2021: "although the apparent detection of hotspots from values of local indicators has been quite widely adopted, it remains fraught with difficulty because adjustment of the inferential basis to accommodate multiple comparisons is not often chosen." So we need to ensure the critical values we use adjust for this. We can use `stats::p.adjust()` to count the number of observations that this statistic identifies as hot spots using different solutions for the multiple comparison problem, such as Bonferroni, False Discovery Rate (`fdr`), or the method developed by Benjamini and Yekutielli (`BY`).

```{r}
p_values_lg <- 2 * pnorm(abs(c(grid_cc$G_hotspot_z)), lower.tail = FALSE)
p_values_lgmc <- cbind(p_values_lg, 
              p.adjust(p_values_lg, "bonferroni"), 
              p.adjust(p_values_lg, "fdr"), 
              p.adjust(p_values_lg, "BY"))
colnames(p_values_lgmc) <- c("None", "Bonferroni", 
                           "False Discovery Rate", "BY (2001)")
apply(p_values_lgmc, 2, function(x) sum(x < 0.05))
```

We can now plot this significant clusters in the map. We will just take the areas identified as significant clusters with the Bonferroni adjustment. First we will do some recoding to identify these cells in the spatial polygon object.

```{r}
# Creates p values for Bonferroni adjustment
grid_cc$G_hotspot_bonf <- p.adjust(p_values_lg, "bonferroni")
# Create character vector based on Z score and p values (since this is a spatial polygon tidier functions such as mutate won´t do the job and have to use base R)
grid_cc$HOTSPOT <- "Not significant"
grid_cc$HOTSPOT[grid_cc$G_hotspot_bonf < 0.05 & grid_cc$G_hotspot_z > 0] <- "Hot spot"
grid_cc$HOTSPOT[grid_cc$G_hotspot_bonf < 0.05 & grid_cc$G_hotspot_z < 0] <- "Cold spot"
table(grid_cc$HOTSPOT)

```

We can now map these values.

```{r, message=FALSE, warning=FALSE}
tmap_mode("view")
map1 <- tm_shape(st_as_sf(grid_cc)) + 
  tm_fill(c("G_hotspot_z"), 
          title="Z scores",
          alpha = 0.5)
map2 <- tm_shape(st_as_sf(grid_cc)) +
  tm_fill(c("HOTSPOT"), 
          title="Hot spots",
          alpha = 0.5)
tmap_arrange(map1, map2, nrow=1)


```

We can see here that not all the micro grid cells with an elevated count of burglaries exhibit significant local dependence. There are only two clusters of burglaries where we see positive spatial dependence. One is located north of Picadilly Gardens in the Northern Quarter of Manchester (an area of trendy bars and shops, and significant gentrification, in proximity to a methadone delivery centre) and the Deansgate area of the city centre (one of the main commercial hubs of the city not far from the criminal courts).

## Local Moran´s I and Moran scaterplot

### Computing the local Moran´s I

The Local Gerdis and Ord statistic, as we noted is a type of local indicator of spatial autocorrelation. There is also a local version of Moran´s I, which allow for the  the decomposition of such as Moran's I, into the contribution of each observation. 

Let's first look at the Moran's scatterplot for our data. Remember from last chapter that a Moran´s scatterplot is plotting the values of the variable of interest, in this case the burglary count on each micro grid, against the spatial lagged value. For creating the Moran´s scatterplot we generally use row standardisation in the weight matrix so that the Y and the X axis are more comparable.

```{r}

lw_queen_rs <-  nb2listw(nb_queen_cc, style='W')
m_plot_burglary <- moran.plot(as.vector(scale(grid_cc$layer)),
                              lw_queen_rs,
                              cex=1, pch=".",
                              xlab="burglary count",
                              ylab="lagged burglary count")

```

Notice how the plot is split in 4 quadrants structured around the centered mean for the value of burglary and its spatial lag. The top right corner belongs to areas that have high level of burglary and are surrounded by other areas that have above the average level of burglary. These are **high-high** locations. The bottom left corner belongs to the **low-low areas**. These are areas with low level of burglary and surrounded by areas with below average levels of burglary. Both the high-high and low-low represent clusters. A high-high cluster is what you may refer to as a *hot spot*. And the low-low clusters represent *cold spots*. 

In the opposite diagonal we have **spatial outliers**. They are not outliers in the standard statistical sense, extreme observations, they are outliers in that they are surrounded by areas that are very unlike them. So you could have *high-low spatial outliers*, areas with high levels of burglary and low levels of surrounding burglary, or *low-high spatial outliers*, areas that have themselves low levels of burglary (or whatever else we are mapping) and that are surrounded by areas with above average levels of burglary. This would suggest pockets of non-stationarity. @Anselin_1996 warns these can sometimes surface as a consequence of problems with the specification of the weight matrix.

The slope of the line in the scatterplot gives you a measure of the global spatial autocorrelation. As we noted earlier, the local Moran´s I  help you to identify what are the locations that weigh more heavily in the computation of the global Moran´s I. The object generated by the `moran.plot()` function includes a measure of this leverage, a hat value, for each location in your study area and we can plot these values if we want to geographically visualise those locations:

```{r, warning=FALSE, message=FALSE}
# We extract the influence measure and place it in the sp object
grid_cc$lm_hat <- m_plot_burglary$hat
# Then we can plot this measure of influence
tm_shape(grid_cc) + tm_fill("lm_hat")

```

To compute the local Moran we can use a function from the `spdep` package: `localmoran()`.

```{r}
locm_burglary <- localmoran(grid_cc$layer, lw_queen_rs,
                            alternative = "two.sided")
```

As with the local $G*$ it is necessary to adjust for multiple comparisons. The unadjusted p value for the test is stored in the fifth column of the object of class `localmoran` that we generated with the `localmoran` function. As before we can extract this element and pass it as an argument to the `p.adjust()` function in order to obtain p values with some method of adjustment for the problem of multiple comparisons.

```{r}
p_values_lm <- locm_burglary[,5]
sum(p_values_lm < 0.05)

```
If we do not adjust for multiple comparisons, then, there would be 33 areas with a significant p value.

```{r}
bonferroni_lm <- p.adjust(p_values_lm, "bonferroni")
sum(bonferroni_lm < 0.05)
```

These get reduced to 11 once we apply the Bonferroni correction. It is also possible to use conditional permutation for inference purposes, instead of the analytical methods used above. In this case we would need to invoke `spdep::localmoran_perm()` function. Here we would need an additional argument `nsim` setting the number of simulations. @Bivand_2018 suggest that "increasing the number of draws beyond 999 has no effect" (p. 741). The `iseed` argument ensures we use the same random seed and get reproducible results. As before adjusting for multiple comparisons notably reduces the number of significant clusters.

```{r}
locm_burglary_perm <- localmoran_perm(grid_cc$layer, 
                                      lw_queen_rs, 
                                      nsim=499, 
                                      alternative="two.sided", 
                                      iseed=1)
p_values_lmp <- locm_burglary_perm[,5]
sum(p_values_lmp < 0.05)
bonferroni_lmp <- p.adjust(p_values_lmp, "bonferroni")
sum(bonferroni_lmp < 0.05)
```

### Creating a LISA map

In order to produce the LISA map we need to do some previous work. First we are going to create some new variables that we are going to need.

First we scale the variable of interest. As noted earlier, when we scale burglary what we are doing is re-scaling the values so that the mean is zero. We use `scale()`, which is a generic function whose default method centers and/or scales the variable. We've also added `as.vector()` to the end, to make sure that the data type we get out of this is a vector, that maps neatly into our `sp` object. Then we will also store the adjusted p values into a variable within this object.

```{r}
# Scale the count of burglary
grid_cc$s_burglary <- scale(grid_cc$layer) %>% as.vector()
# Creates p values for Bonferroni adjustment
grid_cc$localmp_bonf <- p.adjust(p_values_lmp, "bonferroni")

```

To produce the LISA maps we also need to generate a **spatial lag**: the average value of the burglary count in the areas that are considered neighbours of each LSOA.  For this we need our `listw` object, which is the  object created earlier, when we generated the list with weights using row standardisation. We then pass this `listw` object into the `lag.listw()` function, which computes  the spatial lag of a numeric vector using a `listw` sparse representation of a spatial weights matrix. 

```{r}
#create a spatial lag variable and save it to a new column
grid_cc$lag_s_burglary <- lag.listw(lw_queen_rs, grid_cc$s_burglary)
```

Make sure to check the summaries to ensure nothing weird is going on

```{r, eval=FALSE}

summary(grid_cc$s_burglary)
summary(grid_cc$lag_s_burglary)
```

We are now going to create a new variable to identify the quadrant in which each observation falls within the Moran Scatter plot, so that we can tell apart the high-high, low-low, high-low, and low-high areas. We will only identify those that are significant according to the p value that was provided by the local moran function adjusted for multiple comparisons. The data we need for each observation, in order to identify whether it belongs to the high-high, low-low, high-low, or low-high quadrants are the standardised burglary score, the spatial lag score, and the p-value. 

Essentially all we'll be doing, is assigning a variable values based on where in the plot it is. So for example, if it's in the upper right, it is high-high, and has values larger than 0 for both the burglary and the spatial lag values.  It it's in the upper left, it's low-high, and has a value larger than 0 for the spatial lag value, but lower than 0 on the burglary value. And so on, and so on. Here's an image to illustrate:

![](img/moran_plot_annotate.png)

So let's first initialise this variable. In this instance we are creating a new column in the `sp` object and calling it "quad_sig". 


```{r}
grid_cc$quad_sig[grid_cc$localmp_bonf >= .05] <- "Not significant"
grid_cc$quad_sig[grid_cc$s_burglary > 0 & 
                   grid_cc$lag_s_burglary > 0 &  
                   grid_cc$localmp_bonf < 0.05] <- "high-high"
grid_cc$quad_sig[grid_cc$s_burglary < 0 & 
                   grid_cc$lag_s_burglary < 0 & 
                   grid_cc$localmp_bonf < 0.05] <- "low-low"
grid_cc$quad_sig[grid_cc$s_burglary < 0 & 
                   grid_cc$lag_s_burglary > 0 & 
                   grid_cc$localmp_bonf < 0.05] <- "low-high"
grid_cc$quad_sig[grid_cc$s_burglary > 0 & 
                   grid_cc$lag_s_burglary < 0 & 
                   grid_cc$localmp_bonf < 0.05] <- "high-low"

     
```


Now we can have a look at what this returns us: 

```{r}

table(grid_cc$quad_sig)

```

So the 9 significant clusters we found split into 5 high-high and 4 low-high. 
Let's put them on a map, using the standard colours used in this kind of maps:

```{r, echo=FALSE}
tmap_mode("view")
```

```{r, warning=FALSE,message=FALSE}

tm_shape(grid_cc) + 
  tm_fill("quad_sig",
          palette= c("red","blue","white"),
          labels = c("High-High","Low-High", "non-significant"),
          alpha=0.5) +
  tm_borders(alpha=.5) +
  tm_layout(frame = FALSE,
            legend.position = c("right", "bottom"), 
            legend.title.size = 0.8,
            legend.text.size = 0.5,
            main.title = "LISA with the p-values",
            main.title.position = "centre",
            main.title.size = 1.2)
```

## Local spatial heterokedasticity

## A quick overview of `rgeoda`

## Epidemiological tools for detection of clusters in lattice data

### Oppenshaw´s Geographical Analysis Machine and `DCluster` 

The package `DCluster` (@Gomez_2005) was one of the first to introduce tools to assess spatial clusters. This package precedes `sf` and it likes the data to be stored in a data frame with at least four columns: observed number of cases, expected number of cases, and the longitude and latitude. `DCluster`, like many packages that are influenced by the epidemiological literature, take as key inputs the observed cases (in criminology this will be criminal occurrences) and expected cases. In epidemiology expected cases are often standardised to account for population heterogeneity. In crime analysis this is not as common (although in some circumstances it should be!).  

Straight away we see the problem we encounter here. The criminology of place emphasises small places, but for this level of aggregation (street segments or micro grid cells) we won´t have census variables indexing the population. This means we need to (1) either compromise and look for clusters at a level of aggregation for which we have some population measure or (2) get creative an try to find other ways of solving this problem. Increasingly environmental criminologists are trying to find ways of taking this second route (see @Andresen_2021). For this illustration, we will take the first one. Although it is important to warn we should try to stick whenever possible to the level of the spatial process.

For this example we will the  expected counts are computed using the overall incidence ratio (i.e., total number of cases divided by the total population: in this case the number of residential dwellings in the area).

```{r}
burglary_lsoa$b_expected <- burglary_lsoa$dwellings * sum(burglary_lsoa$burglary) / sum(burglary_lsoa$dwellings)

```

This package requires as well the centroids for the polygons we are using. To extract this we can use `sf::st_centroid()` with `sf::st_coordinates()`. All the GEOS functions underlying `sf` need projected coordinates to work properly, so we need to ensure an adequate projection (which we did when we loaded the data at the outset of the chapter):

```{r, warning=FALSE}
# Get coordinates for centroids
nh_centroid <- st_centroid(burglary_lsoa) %>%
  st_coordinates(nh_centroid)
# Place the coordinates as vectors in our sf dataframe
burglary_lsoa$x <- nh_centroid[,1]
burglary_lsoa$y <- nh_centroid[,2]

```

For convenience we will just place these four columns in a new object. It´s not clear from the documentation, some of the code will only work if you initialise the data using these column names:

```{r}
lsoas <- data.frame(Observed = burglary_lsoa$burglary,
                   Expected = burglary_lsoa$b_expected,
                    Population = burglary_lsoa$dwellings,
                    x = burglary_lsoa$x,
                    y = burglary_lsoa$y)
```

`DCluster` includes several tests for assessing the heterogeneity of the relative risks. A chi square test can be run to assess global differences between observed and expected cases.

```{r}
chtest <- achisq.test(Observed~offset(log(Expected)), 
                      as(lsoas, "data.frame"), 
                      "multinom", 
                      999)
chtest
```
It also includes the Pottoff and Withinghill test of homogeneity. The alternative hypothesis of this test is that the observed cases are distributed following a negative binomial distribution (Bivand_2013).

```{r}
pwtest <- pottwhitt.test(Observed~offset(log(Expected)),
                         as(lsoas, "data.frame"),
                         "multinom", 999)
oplus <- sum(lsoas$Observed)
1 - pnorm(pwtest$t0, oplus * (oplus - 1), sqrt(2 * 100 * oplus * (oplus - 1)))
```


```{r}
oppenheim_burglary <- opgam(data = as(lsoas, "data.frame"), 
                   radius = 50, step = 10, alpha = 0.002)
oppenheim <- st_as_sf(oppenheim_burglary, 
                      coords = c("x", "y"),
                      crs = 27700)
oppenheim <- st_transform(oppenheim, crs = 4326)
mapview(list(oppenheim, burglary_lsoa),
        layer.name = c("cacadevaca", "Manchester LSOAs"))

```


```{r}
## Set Parameters
pop.upper.bound <- 0.5
n.simulations <- 999
alpha.level <- 0.05
plot <- TRUE
poisson <- kulldorff(nh_centroid, lsoas$Observed, lsoas$Population, lsoas$Expected,
          pop.upper.bound, n.simulations, alpha.level, plot)
cluster <- poisson$most.likely.cluster$location.IDs.included
cluster_sf <- burglary_lsoa[cluster,]
mapview(list(cluster_sf, manchester),
        layer.name = c("Kulldorf", "Manchester"))

```





```{r}


mle <- calculate.mle(as(lsoas, "data.frame"), model = "negbin")

thegrid <- as(lsoas, "data.frame")[, c("x", "y")]
knresults <- opgam(data = as(lsoas, "data.frame"),
                   thegrid = thegrid, alpha = 0.02, iscluster = kn.iscluster,
                   fractpop = 0.05, R = 99, model = "negbin",
                   mle = mle)
kulldorff_scan <- st_as_sf(knresults, 
                      coords = c("x", "y"),
                      crs = 27700)
kulldorff_scan <- st_transform(kulldorff_scan, crs = 4326)
mapview(list(kulldorff_scan, burglary_lsoa),
        layer.name = c("r", "Manchester LSOAs"))
```

## Assessing clusters with point pattern data

## Further reading
