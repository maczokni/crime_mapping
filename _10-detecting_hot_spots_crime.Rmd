# Chapter 10: Assessing local clusters and repeats

## Introduction

[[[Bla bla bla hot spots of crime]]]

In the previous chapter we explored ways to visualise the concentration of crime incidents in particular location. We used tesselation and kernel density estimation as a way to do this. As noted by @Chainey_2020:

>"Hot spot maps generated using KDE are useful for showing where crime concentrates but may fail to determine unambiguosly what is hot (in hot spot analysis terms) from what is not hot. That is, a KDE hot spot map may fail to separate the significant spatial concentrations of crime from those spatial distributions of less interest, or from random variation"

There are a number of tools that have been developed to detect local clusters. In crime analysis is common the use of the local $Gi^*$ statistic. Whereas in spatial epidemiology a number of techniques have been developed for detecting areas with an unusually high risk of disease. Although originally developed in the context of studying disease, the techniques can also be applied to the study of crime. Most of the methods in spatial epidemiology are based on the idea of "moving windows", such as the spatial scan statistic developed by @Kulldorff_1997. Several packages bring this scan statistics to R. There are also other packages that use different algorithms to detect disease clusters, such as AMOEBA, that is based in the Getis-Ord algorithm. As we will see we are spoiled for choice in this regard.

In crime analysis is also relevant the study of near repeat victimisation. bla bla bla...

For this section we will need the following packages:

```{r}

library(dplyr)

# Packages for handling spatial data and for geospatial carpentry
library(sf)
library(raster)
library(spdep)

# Packages for detecting clusters

library(DCluster)
library(SpatialEpi)

library(smerc)
library(surveillance)
library(DClusterm)

# Packages for detecting near repeat victimisation
## You will need to install the following package from github
## You can use the following code for it: 
## remotes::install_github("wsteenbeek/NearRepeat", build_vignettes = TRUE)
library(NearRepeat)

# Packages for visualisation and mapping
library(tmap)
library(leaflet)
library(mapview)
library(RColorBrewer)
```

## Break and enter in Toronto

For this chapter we travel to Toronto. We will be looking at crime data provided by their Police Service through their Public Safety Data Portal (https://data.torontopolice.on.ca/). We will explore residential break and enter offences for 2019. Beware there is also geomasking going on here. The open data does not include the exact coordinates for these events. For privacy reasons they have been geocoded to the next street intersection. When aggregating the incidents at the neighbourhoud level, as we will do later in this chapter, this may be less problematic. But if you were to be interested in the spatial point pattern level of resolution, you would be coming across artificially created "hot spots" in those street intersections. That´s a problem with most of these open data on crime, but for illustration purposes of the techniques it is ok to use this data - as long as we remember these caveats. 

The cartographic boundary for the city comes from the census office of Canada. We will also need the neighbourhood boundaries for Toronto, which we obtain from the Toronto open data portal (https://open.toronto.ca/). These data is free to redistribute, so for convenience we have assembled it and can be read from our GitHub repository with the following code:


```{r, message = FALSE, warning = FALSE}
toronto <- st_read("data/toronto_c.geojson")
break_enter <- st_read("data/break_enter.geojson") %>%
  filter((premises_t == "Apartment" | premises_t == "Commercial") & reportedye == "2019" | reportedye == "2018")
neighbourhoods <- st_read("data/nh_profiles_short.geojson")
  
```


We can now plot it:

```{r}

tm_shape(toronto) +
  tm_polygons(alpha = 0.3) +
  tm_shape(neighbourhoods) +
  tm_polygons(alpha = 0.3) +
  tm_shape(break_enter) +
  tm_dots(size = 0.01) +
   tm_layout(main.title = "Break and enter in Toronto", 
            main.title.size = 1 ,
            legend.outside = TRUE,  # Takes the legend outside the main map 
            legend.title.size = 0.8)

```

As you can see, and it is often the case, the data requires some cleaning. Some points lie outside the city boundaries, for this we will use code introduced in Chapter 2, and that´s not the only issue.  We also want to make sure there are no records with the same id, there are some in this dataset. So we will filter the data on the unique ID. For filtering unique rows we can rely on the magic of `duplicated()`. This function gives distinct records for which the values in the selected variable are the same. 

```{r, message=FALSE}

break_enter <- subset(break_enter, !duplicated(break_enter$event_uniq))
# intersection
break_enter_intersect <- st_intersects(toronto, break_enter)
# subsetting
break_enter <- break_enter[unlist(break_enter_intersect ),]

```

## Micro Grid Cells

In chapter 4 we discussed alternatives to our choropleth maps, and one of these was to use a grid map. This is a process whereby a tessalating grid of a shape such as squares or hexagons is overlaid on top of our area of interest, and our points are aggregated to such a grid. We explored this using `ggplot2` package. We also used this approach when discussing quadrat counting with `ppp` objects and using the functionality provided by the `spatstat` package.

Here we can revisit this with another approach, using the `raster` package. First we need to create the grid for the raster using the `raster()` function. The first argument defines the extent for the grid. By using our "toronto" `sf` object we ensure it includes all of it. Then the `resolution` argument simply defines how large we want the cells in this grid to be. You can play around with this argument and see how the results will vary. Then we will use the `rasterize()` function to count the number of points that fall within each cell. This is just a form of quadrant counting as the one we introduced in Chapter 6. The `rasterize()` function takes as the first argument the coordinates of the points. The second argument provides the grid we just created. The `field` argument in this case is assigned a value of `1`  so that each point is counted a unit. Then we use the `fun` argument to specify we want to `"count"` the points within each cell. The `rasterize()` function by default will set at NA all the values where there are no points. We don´t want this to be the case. So that the raster include those cells as zero we use the `background` argument and set it to zero: 

```{r}
# Create empty raster grid
toronto_r <- raster(toronto, resolution = 0.004)
# Fill the grid with the count of incidents within each cell
break_enter_raster <- rasterize(break_enter,
                                toronto_r,
                                field = 1,
                                fun = "count",
                                background = 0)
# Use base R functionality to plot the raster
plot(break_enter_raster)
plot(st_geometry(toronto), border='#00000040', add=T)
```

This kind of map is helpful if you want to get a sense of the distribution over the whole city, but oftenwise from a crime analyst operational point of view the interest is at a larger scale of resolution. We want to deep in and observe the micro-places that may be attracting crime within certain parts of the city. We can do this by focusing on particular neighbourhoods.

Let´s find out the neighbourhood with most events. First, we use `st_intersects()` to count points within polygons and then add this to the "neighbourhoods" `sf` object. Then we use `slice_max()` to select the rows with the highest values. Setting the argument `n` to 3, we get the three neighbourhoods with more incidents. For sparser reporting we previously `select()` only the variable containing the neighbourhood name and the count of break and enter occurrences. Using `as.data.frame()` ensures that we don´t retain the geometry and additional elements of the `sf` object when autoprinting the results.

```{r, message = FALSE}
# add point count to each polygon
neighbourhoods$b_e_count <- lengths(st_intersects(neighbourhoods, break_enter))
# identify the neighbourhood with most incidents
as.data.frame(neighbourhoods) %>% 
  dplyr::select(Neighbourh, b_e_count) %>%
  slice_max(b_e_count, n = 3) 
```

Let´s look at Kensington-Chinatown, a well known and distinctive multicultural part of Toronto which is a noted tourist attraction.

```{r, warning = FALSE, message = FALSE}
# Create sf vector with just Kensington boundaries
kensington <- filter(neighbourhoods, 
                     Neighbourh == "Kensington-Chinatown")
# Subset the break and enter incidents within this area of Toronto
break_enter_intersect <- st_intersects(kensington, break_enter)
break_enter_k <- break_enter[unlist(break_enter_intersect ),]

# Create empty grid for Kensington
kensington_r <- raster(kensington, resolution = 0.0006)
# Produce raster object with the number of incidents within each cell
break_enter_raster_k <- rasterize(break_enter_k,
                                kensington_r,
                                field = 1,
                                fun = "count",
                                background = 0)
# Plot results with base R
plot(break_enter_raster_k)
plot(st_geometry(kensington), border='#00000040', add=T)
```
We don´t have in this raster object incidents outside the boundaries of the neighborhoud, so we could prefer to declare those cells as NAs. We can do that with the `raster::mask()` function.

```{r}
break_enter_raster_k <- mask(break_enter_raster_k, kensington)
plot(break_enter_raster_k)
```

Let´s add some context. We will use `mapview` for it. This is another great package if you want to give your audience some interactivity. It does what is says in the tin: 
>it "provides functions to very quickly and conveniently create interactive visualisations of spatial data. Its main goal is to fill the gap of quick (not presentation grade) interactive plotting to examine and visually investigate both aspects of spatial data, the geometries and their attributes. It can also be considered a data-driven API for the leaflet package as it will automatically render correct map types, depending on the type of the data (points, lines, polygons, raster)." 

With very few lines of code you can get an interactive plot. Here we first will use `RColorBrewer::brewer.pal()` to define a convenient palette and then use `mapview::mapview()` to plot our raster. Below you see how we increase the transparency with the `alpha.region` argument and how we use `col.regions` to change from the default palette to the one we created.

```{r, message=FALSE, warning=FALSE}

my.palette <- brewer.pal(n = 9, name = "OrRd")
mapview(break_enter_raster_k, 
        alpha.regions = 0.7,
        col.regions = my.palette)
```



## Local spatial autocorrelation with lattice data

It is indeed useful for us to be able to assess quantitatively whether crime events cluster in a non-random manner. in the words of Jerry @Ratcliffe_2010, however, "while a global Moran’s I test can show that crime events cluster in a non-random manner, this simply explains what most criminal justice students learn in their earliest classes." (p. 13). For a crime analyst and practitioner, whereas clustering is of interest, learning about the existence and location of local clusters is of paramount importance.

In this section we will learn about local indicators of spatial association (LISA) and show how they allow for the decomposition of global indicators, such as Moran's I, into the contribution of each observation. The LISA statistics serve two purposes. On one hand, they may be interpreted as indicators of local pockets of nonstationarity, or hot spots. On the other hand, they may be used to assess the influence of individual locations on the magnitude of the global statistic and to identify “outliers” (@Anselin_1995). @Anselin_1995 suggests that any LISA satisfies two requirements (p. 94:
"-a) the LISA for each observation gives an indication of the extent of significant spatial clustering of similar values around that observation;
- b) the sum of LISAs for all observations is proportional to a global indicator of spatial association".

### Getting Manchester data and weights 

To demonstrate how to derive these local indicators of spatial correlation, let's go back to using all of Manchester, rather than just the City Centre ward. We want to have enough data to see local variation. We will use the `burglaries_per_lsoa` object that we loaded at the outset of this chapter. Now that we have the data we need to coerce it into a spatial object, as before, for it to work well with the functions we use from the `sp` package, and then generate the weight matrix. Again, what we do here is stuff we did earlier in this chapter. 

```{r, warning=FALSE}

#Coerce sf into sp
burglary_m <- as(burglaries_per_lsoa, "Spatial")
#Generate list of neighbours using the Queen criteria
ln_m <- poly2nb(burglary_m, row.names=burglary_m$lsoa_code)
#Generate list with weights using row standardisation
lw_m <-  nb2listw(ln_m, style='W')

```

### Generating the LISA measures

Let's first look at the Moran's scatterplot for our data:

```{r}
moran.plot(burglary_m$burglary, lw_m)
```

Notice how the plot is split in 4 quadrants. The top right corner belongs to areas that have high level of burglary and are surrounded by other areas that have above the average level of burglary. These are **high-high** locations. The bottom left corner belongs to the **low-low areas**. These are areas with low level of burglary and surrounded by areas with below average levels of burglary. Both the high-high and low-low represent clusters. A high-high cluster is what you may refer to as a *hot spot*. And the low-low clusters represent *cold spots*. 

In the opposite diagonal we have **spatial outliers**. They are not outliers in the standard statistical sense, extreme observations, they are outliers in that they are surrounded by areas that are very unlike them. So you could have *high-low spatial outliers*, areas with high levels of burglary and low levels of surrounding burglary, or *low-high spatial outliers*, areas that have themselves low levels of burglary (or whatever else we are mapping) and that are surrounded by areas with above average levels of burglary.

You can also see here that the positive spatial autocorrelation is more noticeable when we focus on the whole of Manchester city, unlike what we observed when only looked at the city centre. You can check this running the global Moran's I.

```{r}
moran(burglary_m$burglary, lw_m, n=length(ww$neighbours), S0=Szero(ww))
moran_range(lw_m)
moran.mc(burglary_m$burglary, lw_m, nsim=99999)
```

You can see that the global Moran's is 0.32 (for a range of I between -.5 to 1.02) and that is highly significant. There is indeed global spatial autocorrelation, when we look at all of Manchester (not just city centre ward). Knowing this we can try to decompose this, figure out what is driving this global measure.

To compute the local Moran we can use a function from the `spdep` package: `localmoran()`.

```{r}
locm_bm <- localmoran(burglary_m$burglary, lw_m)
summary(locm_bm)
```

The first column provides the summary statistic for the local moran statistic. Being local you will have one for each of the areas. The last column gives you a p value for this statistic. 

### Creating a LISA map

In order to produce the LISA map we need to do some previous work. First we are going to create some new variables that we are going to need.

First we scale the variable of interest. When we scale burglary what we are doing is re-scaling the values so that the mean is zero. We use `scale()`, which is a generic function whose default method centers and/or scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.We've also added `as.vector()` to the end, to make sure that the data type we get out of this is a vector, that maps neatly into our dataframe.

```{r}

burglary_m$s_burglary <- scale(burglary_m$burglary) %>% as.vector()
```

To produce the LISA maps we also need to generate a **spatial lag**: the average value of the burglary count in the areas that are considered neighbours of each LSOA. 

For this we need our `listw` object, which is the `ww` object created earlier, when we generated the list with weights using row standardisation. We then pass this `listw` object into the `lag.listw()` function, which computes  the spatial lag of a numeric vector using a `listw` sparse representation of a spatial weights matrix. 

```{r}
#create a spatial lag variable and save it to a new column
burglary_m$lag_s_burglary <- lag.listw(lw_m, burglary_m$s_burglary)
```

Make sure to check the summaries to ensure nothing weird is going on

```{r}

summary(burglary_m$s_burglary)
summary(burglary_m$lag_s_burglary)
```

We can create a Moran scatter plot so that you see that nothing has changed apart from the scale in winch we are using the variables. The observations that are influential are highlighted in the plot as you can see.

```{r}
x <- burglary_m$s_burglary
y <- burglary_m$lag_s_burglary
xx <- data_frame(x,y)
moran.plot(x, lw_m)
```

We are now going to create a new variable to identify the quadrant in which each observation falls within the Moran Scatter plot, so that we can tell apart the high-high, low-low, high-low, and low-high areas. We will only identify those that are significant according to the p value that was provided by the local moran function.

Before we get started, let's quickly review the tools we will use. 

All our data is in this "burglary_m" dataframe. This has a variable for the LSOA code ("code"), a variable for the number of burglaries ("burglary"), and then also the two variables we created, the scaled measure of burglary ("s_burglary"), and the spatial lag measure ("lag_s_burglary").

We also have our "locm_bm" object, which we created with the "localmoran()" function, that has calculated a variety of measures for each of our observations, which we explored with the `summary()` function. You can see (if you scroll up) that the 5th element in this object is the p-value ("Pr(z > 0)"). To call the nth element of an object, you can use the square brackets after its name. So to return the nth column of thing, you can use `thing[,n]`. Again this should not be new to you, as we've been doing this sort of thing for a while. 

So the data we need for each observation, in order to identify whether it belongs to the high-high, low-low, high-low, or low-high quadrants are the standardised burglary score, the spatial lag score, and the p-value. 

Essentially all we'll be doing, is assigning a variable values based on where in the plot it is. So for example, if it's in the upper right, it is high-high, and has values larger than 0 for both the burglary and the spatial lag values.  It it's in the upper left, it's low-high, and has a value larger than 0 for the spatial lag value, but lower than 0 on the burglary value. And so on, and so on. Here's an image to illustrate:

![](img/moran_plot_annotate.png)


So let's first initialise this variable. In this instance we are creating a new column in the "burglary_m" dataframe and calling it "quad_sig". 

We are using the `mutate()` function from the `dplyr` package to create our new variable, just as we have in previous labs. 

We also use nested `ifelse()` statements. Nested `ifelse()` just means that it's an `ifelse()` inside another `ifelse()` statement. To help us with these sorts of situations is the `ifelse()` function. We saw this with the previous exercises, but I'll describe it brielfy again. It allows us to conditionally assign some value to some variable. The structure of the function is so that you have to pass it a condition, then a value to assign if the condition is true, and then another value if the condition is false. You are basically using this function to say: "if this condition is true, do first thing, else, do the second thing". It would look something like this:

```{r, eval=FALSE}

dataframe$new_variable <- ifelse(dataframe$some_numeric_var < 100, "smaller than 100", "not smaller than 100")

```

When nesting these, all you do is put another condition to check in the "thing to do if false", so it checks all conditions. So in the first instance we check if the value for burglary is greater than zero, and the value for the lag is greater than zero, and the p-value is smaller than our threshold of 0.05. If it is, then this should belong to the "high-high" group. If any one of these conditions is not met, then we move into the 'thing to do if false' section, where we now check again another set of criteria, and so on and so on. If none of these are met, we assign it the non-significant value: 


```{r}
burglary_m <- st_as_sf(burglary_m) %>% 
  mutate(quad_sig = ifelse(burglary_m$s_burglary > 0 & 
                              burglary_m$lag_s_burglary > 0 & 
                              locm_bm[,5] <= 0.05, 
                     "high-high",
                     ifelse(burglary_m$s_burglary <= 0 & 
                              burglary_m$lag_s_burglary <= 0 & 
                              locm_bm[,5] <= 0.05, 
                     "low-low", 
                     ifelse(burglary_m$s_burglary > 0 & 
                              burglary_m$lag_s_burglary <= 0 & 
                              locm_bm[,5] <= 0.05, 
                     "high-low",
                     ifelse(burglary_m$s_burglary <= 0 & 
                              burglary_m$lag_s_burglary > 0 & 
                              locm_bm[,5] <= 0.05,
                     "low-high", 
                     "non-significant")))))
```


(Note we had to wrap our data in a `st_as_sf()` function, to convert back to `sf` object). 

Now we can have a look at what this returns us: 

```{r}

table(burglary_m$quad_sig)

```

This looks like a lot of non-significant results. We want to be sure this isn't an artefact of our code but is true, we can check how many values are under 0.05:


```{r}

nrow(locm_bm[locm_bm[,5] <= 0.05,])

```

We can see that only 23 areas have p-values under 0.05 threshold. So this is in line with our results, and we can rest assured. Well, this is exciting, but where are these regions?

Let's put'em on a map, using the standard colours used in this kind of maps:

```{r, echo=FALSE}
tmap_mode("view")
```


```{r}

tm_shape(burglary_m) + 
  tm_fill("quad_sig",
          palette= c("red","blue","white"),
          labels = c("High-High","Low-Low", "non-significant"),
          alpha=0.5) +
  tm_borders(alpha=.5) +
  tm_layout(frame = FALSE,
            legend.position = c("right", "bottom"), 
            legend.title.size = 0.8,
            legend.text.size = 0.5,
            main.title = "LISA with the p-values",
            main.title.position = "centre",
            main.title.size = 1.2)
```

So how do we interpret these results? Well keep in mind: 

- The LISA value for each location is determined from its individual contribution to the global Moran's I calculation.
- Whether or not this value is statistically significant is assessed by comparing the actual value to the value calculated for the same location by randomly reassigning the data among all the areal units and recalculating the values each time (the Monte Carlo simulation approach discussed earlier).

So essentially this map now tells us that there was statistically significant moderate clustering in burglaries in Manchester. If you were familiar with the city you could see they are located around the city centre, areas of university student residencies, and some pockets of social exclusion.  When reporting your results, report at least the Moran’s I test value and the p value.  So, for this test, you should report Moran’s I = 0.32, p < .001. Including the LISA cluster map is also a great way of showing how the attribute is actually clustering.

## Local Getis-Ord

The maps above and the ones we introduced in chapter 6 produced through kernel density estimation are helpful for visualising the varying intensity of crime across the study region of interest. However, as we noted in Chapter 6, it may be hard to discern random variation from clustering of crime incidents simply using these techniques. There are a number of statistical tools that have been developed to extract the signal from the noise, to determine what is hot from random variation, and that were originally designed to detect clusters of disease.

In crime analysis is common to use a technique developed by Arthur Getis and JK Ord in the early 1990s (@Getis_1992 and @Ord_1995), partly because it was implemented in the widely successful ArcGIS as part of their hot spot analysis tool. This statistic provides a measure of spatial dependence at the local level. They are not testing homogeneity, they are testing dependence of a given attribute around neighbouring areas (which could be operationalised as micro cells in a grid, as above, or could be some form of administrative areas). 

There are two variants of the local Getis-Ord statistic. The local $G$ is computed a ratio of the weighted average of the values of the attribute of interest in the neighboring locations, not including the value at the location. It is generally used for spread and diffusion studies. The local $G^*$ includes the value at the location in both numerator and denominator. It is more generally used for clustering studies. High positive values indicate the possibility of a local cluster of high values of the variable being analysed ("hot spot"), very low relative values a similar cluster of low values ("cold spot"). "A value larger than the mean (or, a positive value for a standardized z-value) suggests a High-High cluster or hot spot, a value smaller than the mean (or, negative for a z-value) indicates a Low-Low cluster or cold spot." (@Anselin_2020a) 

For statistical inference, a Bonferroni-type test is suggested in the the papers by Getis and Ord, where tables of critical values are included. The critical values for the 95th percentile under the assumptions discussed by @Ord_1995 are for $n$=30: 2.93, $n$=50: 3.08, $n$=100: 3.29, $n$=500: 3.71, and for $n$=1000: 3.89. @Anselin_2020a suggest that this analytical approximation may not be reliable in practice and that conditional permutation is advisable. Inference with these local measures are also complicated by problems of multiple comparisons, so it is typically advisable to use some form of adjustment to this problem.

The first step in computing the Getis and Ord statistics involves defining the neighbours of each area. And to do that first we have to turn each cell in our grid into a polygon, so we basically move from the raster to the vector model by virtue of using `raster::rasterToPolygons`. 

```{r}
# Move to vector model
getisgrid <-  rasterToPolygons(break_enter_raster_k)

```

What is a neighbour and the code related to defining neighbours is a topic we explain in much greater detail in chapter 10 when we discuss more generally related measures of dependence. For now, just take the code at face value and understand we are simply looking at each cell and establishing that immediately adjacent cells are its neighbours. Only to indicate that `include.self()` ensures we get the $G^*$ variation of the local Getis-Ord test, the second variation we mentioned above.

```{r}
neighbors <-  poly2nb(getisgrid)
weighted_neighbors <- nb2listw(include.self(neighbors), zero.policy=T)


```

Now that we have the data ready we can, analytically, compute the statistic using the `localG()` function. We can then add the computed local $Gi^*$ to each cell.

```{r, message=FALSE, warning=FALSe}

# Perform the local G analysis (Getis-Ord GI*)
getisgrid$HOTSPOT_z <- as.vector(localG(getisgrid$layer, weighted_neighbors))


```

We can then see the produced z scores:

```{r}
summary(getisgrid$HOTSPOT_z)

```

How do you interpret these? They are z scores. So you expect large absolute values to index the existence of cold spots (if they are negative) or hot spots (if they are positive). Our sample size is 475 cells and there are only a very small handful of cells with values that reach the critical value in our dataset.

A known problem with this statistic is that of multiple comparisons. As noted by @Pebesma_2021: "although the apparent detection of hotspots from values of local indicators has been quite widely adopted, it remains fraught with difficulty because adjustment of the inferential basis to accommodate multiple comparisons is not often chosen." So we need to ensure the critical values we use adjust for this. We can use `stats::p.adjust()` to count the number of observations that this statistic identifies as hot spots using different solutions for the multiple comparison problem, such as Bonferroni, False Discovery Rate, or the method developed by Benjamini and Yekutielli (BY).

```{r}
p_values <- 2 * pnorm(abs(c(getisgrid$HOTSPOT_z)), lower.tail = FALSE)
p_values_mc <- cbind(p_values, 
              p.adjust(p_values, "bonferroni"), 
              p.adjust(p_values, "fdr"), 
              p.adjust(p_values, "BY"))
colnames(p_values_mc) <- c("None", "Bonferroni", 
                           "False Discovery Rate", "BY (2001)")
apply(p_values_mc, 2, function(x) sum(x < 0.05))
```

We can now plot this significant clusters in the map. First we will do some recoding to identify these cells in the spatial polygon object.

```{r}
# Creates p values for Bonferroni adjustment
getisgrid$HOTSPOT_bonf <- p.adjust(p_values, "bonferroni")
# Create character vector based on Z score and p values (since this is a spatial polygon tidier functions such as mutate won´t do the job and have to use base R)
getisgrid$HOTSPOT <- "Not significant"
getisgrid$HOTSPOT[getisgrid$HOTSPOT_bonf < 0.05 & getisgrid$HOTSPOT_z > 0] <- "Hot spot"
getisgrid$HOTSPOT[getisgrid$HOTSPOT_bonf < 0.05 & getisgrid$HOTSPOT_z < 0] <- "Cold spot"
table(getisgrid$HOTSPOT)

```

We can now map these values.

```{r}
tmap_mode("plot")
map1 <- tm_shape(st_as_sf(getisgrid)) + 
  tm_fill(c("HOTSPOT_z"), 
          title="Z scores")
map2 <- tm_shape(st_as_sf(getisgrid)) +
  tm_fill(c("HOTSPOT"), 
          title="Hot spots")
tmap_arrange(map1, map2, nrow=1)


```

## Scan Statistics

bla bla bla

```{r}
names(neighbourhoods)
```

compute rate

```{r, eval = FALSE}

neighbourhoods <- neighbourhoods %>%
  mutate(b_e_rate = b_e_count / as.numeric(dwellings))
```

What´s going on? Well, that´s the thing with real life data. There´s always something that may drive you insane. If you use `class()` on the column with the "dwellings" you will see that the data are stored as `character` class. What is more you see commas separating some of the digits. R will coerce this into NAs. So we need to do some tidying beforehand. We will turn the vector into numeric using `as.numeric()` and we will use `gsub()` to replace the commas (`","`) with nothingness (`""`).

```{r}
neighbourhoods$dwellings <- as.numeric(gsub(",", "", neighbourhoods$dwellings))

```

Now we can compute the rate and do a quick map.

```{r, message=FALSE}
neighbourhoods <- neighbourhoods %>%
  mutate(b_e_rate = (b_e_count / as.numeric(dwellings)) * 1000)
current_style <- tmap_style("col_blind")
tm_shape(neighbourhoods) + 
  tm_fill("b_e_rate", style="headtails", title = "Break and Enter per 1000 dwellings (2018-2019") +
  tm_layout(legend.position = c("left", "top"), 
            legend.title.size = 0.7,
            legend.text.size = 0.5)
```

The package `DCluster` was one of the first to introduce tools to assess spatial clusters. This package, like many that are influenced by the epidemiological literature, take as key inputs the observed cases (in criminology this will be criminal occurrences) and expected cases. In epidemiology expected cases are often standardised to account for population heterogeneity. In crime analysis this is not as common (although in some circumstances it should be!). For this example we will the  expected counts are computed using the overall incidence ratio (i.e., total number of cases divided by the total population).

```{r}
neighbourhoods$b_e_expected <- neighbourhoods$dwellings * sum(neighbourhoods$b_e_count) / sum(neighbourhoods$dwellings)

```

This package requires as well the centroids for the polygons we are using. To extract this we can use `sf::st_centroid()` with `sf::st_coordinates()`. All the GEOS functions underlying `sf` need projected coordinates to work properly, so we need to reproject first to an adequate projection:

```{r}
# Reproject
neighbourhoods <- st_transform(neighbourhoods, crs = 26717)
# Get coordinates for centroids
nh_centroid <- st_centroid(neighbourhoods) %>%
  st_coordinates(nh_centroid)
# Place the coordinates as vectors in our sf dataframe
neighbourhoods$x <- nh_centroid[,1]
neighbourhoods$y <- nh_centroid[,2]

```


bla bla bla

```{r}
chtest <- achisq.test(b_e_count~offset(log(b_e_expected)), 
                      as(as.data.frame(neighbourhoods), "data.frame"), 
                      "multinom", 
                      999)
chtest
```
```{r}
pwtest <- pottwhitt.test(b_e_count~offset(log(b_e_expected)),
                         as(as.data.frame(neighbourhoods), "data.frame"),
                         "multinom", 999)
oplus <- sum(neighbourhoods$b_e_count)
1 - pnorm(pwtest$t0, oplus * (oplus - 1), sqrt(2 * 100 * oplus * (oplus - 1)))
```

```{r}
nh_scans <- data.frame(Observed = neighbourhoods$b_e_count)
nh_scans <- cbind(nh_scans, Expected = neighbourhoods$b_e_expected)
nh_scans <- cbind(nh_scans, x = neighbourhoods$x,
                  y = neighbourhoods$y)
```



```{r}
sidsgam <- opgam(data = as(nh_scans, "data.frame"), 
                   radius = 30, step = 10, alpha = 0.002)
gampoints <- SpatialPoints(sidsgam[, c("x", "y")] * 1000, CRS("+proj=utm +zone=18 +datum=NAD27"))
library(rgdal)
ll <- CRS("+proj=longlat +datum=NAD27")
gampoints <- spTransform(gampoints, ll)
gam.layout <- list("sp.points", gampoints)
```




```{r}


mle <- calculate.mle(as(nh_scans, "data.frame"), model = "negbin")
thegrid <- as(as.data.frame(neighbourhoods), "data.frame")[, c("x", "y")]
knresults <- opgam(data = as(nh_scans, "data.frame"),
                   thegrid = thegrid, alpha = 0.05, iscluster = kn.iscluster,
                   fractpop = 0.15, R = 99, model = "negbin",
                   mle = mle)
class(knresults)
```

