# Chapter 7: Detecting hot spots of crime and repeats

## Introduction

In the previous chapter we explored ways to visualise the concentration of crime incidents in particular location. We used tesselation and kernel density estimation as a way to do this. As noted by @Chainey_2020:

>"Hot spot maps generated using KDE are useful for showing where crime concentrates but may fail to determine unambiguosly what is hot (in hot spot analysis terms) from what is not hot. That is, a KDE hot spot map may fail to separate the significant spatial concentrations of crime from those spatial distributions of less interest, or from random variation"

There are a number of tools that have been developed to detect local clusters. In crime analysis is common the use of the local $Gi^*$ statistic. Whereas in spatial epidemiology a number of techniques have been developed for detecting areas with an unusually high risk of disease. Although originally developed in the context of studying disease, the techniques can also be applied to the study of crime. Most of the methods in spatial epidemiology are based on the idea of "moving windows", such as the spatial scan statistic developed by @Kulldorff_1997. Several packages bring this scan statistics to R. There are also other packages that use different algorithms to detect disease clusters, such as AMOEBA, that is based in the Getis-Ord algorithm. As we will see we are spoiled for choice in this regard.

In crime analysis is also relevant the study of near repeat victimisation. bla bla bla...

For this section we will need the following packages:

```{r}

library(dplyr)

# Packages for handling spatial data and for geospatial carpentry
library(sf)

# Packages for detecting clusters
library(DCluster)
library(SpatialEpi)
library(AMOEBA)
library(smerc)
library(surveillance)
library(DClusterm)

# Packages for detecting near repeat victimisation
## You will need to install the following package from github
## You can use the following code for it: 
## remotes::install_github("wsteenbeek/NearRepeat", build_vignettes = TRUE)
library(NearRepeat)

# Packages for visualisation and mapping
library(tmap)
```

## Spatial scan statistics

### Getting the data

For this chapter we travel to Toronto. We will be looking at crime data provided by their Police Service through their Public Safety Data Portal. We will be looking at serious traffic collisions in 2018. The source of the data comes from police reports where an officer attended an event related to a traffic collision. Please note that this dataset does not include all traffic collision events. The data only includes events where a person sustained a major or fatal injury in a traffic collision 
event The cartographic boundary for the city comes from the census office of Canada. The data can be read from our GitHub repository with the following code:

```{r, message = FALSE, warning = FALSE}
toronto <- st_read("https://raw.githubusercontent.com/maczokni/crime_mapping/master/data/toronto_c.geojson")
traffic <- st_read("https://raw.githubusercontent.com/maczokni/crime_mapping/master/data/traffic.geojson")

tm_shape(toronto) +
  tm_polygons(alpha = 0.3) +
  tm_shape(traffic) +
  tm_dots(size = 0.01) +
   tm_layout(main.title = "Serious traffic collisions in Toronto", 
            main.title.size = 1 ,
            legend.outside = TRUE,  # Takes the legend outside the main map 
            legend.title.size = 0.8)

```

As you can see, and it is often the case, the data requires some cleaning. Some points lie outside the city boundaries and that´s not the only issue. First there are many rows that share the same event unique id because the rows represent persons involved, not accidents. In typical crazy administrative fashion that you are likely to encounter in real life datasets the "unique" ids are said by the data documentation not to be unique and that "it may repeat year over year". So we will filter the data on the unique ID and the date of the collision hoping this will give us unique traffic collisions. For filtering unique rows we can rely on the magic of `dplyr::distinct()`. This function gives distinct records for which all attributes and geometries are distinct. To make it easier to operate in our case we will first get rid of all unneccesary variables using `select`.

```{r}

traffic <- select(traffic, ACCNUM, DATE)
traffic <- distinct(traffic_u)

```

We are now left with just 6002 records.

## Micro Grid Cells

[[[Is this redundant?]]]

In chapter 4 we discussed alternatives to our choropleth maps, and one of these was to use a grid map. This is a process whereby a tessalating grid of a shape such as squares or hexagons is overlaid on top of our area of interest, and our points are aggregated to such a grid. 


@Chainey_2021 recommend


We explored this using `ggplot2` package. Here we can revisit this with another approach, using the `raster` package: 

```{r}
library(raster)


pixelsize <-  0.01
box <-  (extent(ldn) / pixelsize) * pixelsize
template = raster(box, crs = 4326,
	nrows = (box@ymax - box@ymin) / pixelsize, 
	ncols = (box@xmax - box@xmin) / pixelsize)

btp_crimes$PRESENT <-  1
btp_crimes_raster <- rasterize(btp_crimes, template, field = 'PRESENT', fun = sum)

plot(btp_crimes_raster)
plot(st_geometry(ldn), border='#00000040', add=T)
```

## Getis-Ord GI*

One major issue with the above, which is another issue associated with kernel density estimation is that the choice of a threshold for what is and is not a hot spot is arbitrary, making use of KDE imprecise and subject to misinterpretation.

One commonly used technique to work around that issues was developed by Arthur Getis and JK Ord in the early 1990s. When you develop an algorithm, you get to name it after yourself, so this is called the Getis-Ord GI* statistic.

This statistic is based on the observation that the distribution of differences in crime intensity between neighbouring areas across a map will follow a normal curve. Therefore, the amount of difference between an area and its neighbors can then be converted to a z-score reflecting the number of standard deviations (σ) that the crime level in an area differs from neighborhood normal. Areas with high z-scores (indicating a crime significantly above the mean) are identified as hot spots, while areas with low z-scores are identified as cold spots.

```{r}



getisgrid <-  rasterToPolygons(btp_crimes_raster)

# Create the list of neighbors

library(spdep)
neighbors <-  poly2nb(getisgrid)
weighted_neighbors = nb2listw(neighbors, zero.policy=T)

# Perform the local G analysis (Getis-Ord GI*)

getisgrid$HOTSPOT = as.vector(localG(getisgrid$layer, weighted_neighbors))

# Color the grid cells based on the z-score

breaks = quantile(getisgrid$HOTSPOT, na.rm = TRUE)
palette=c("#0000FF80", "#8080FF80", "#FFFFFF80", "#FF808080", "#FF000080")
col = palette[cut(getisgrid$HOTSPOT, breaks)]

# Plot

plot(getisgrid, col=col, border=NA)
plot(st_geometry(ldn), border="gray", add=T)

```



Or on a leaflet map to aid interpretation: 

```{r}

pal <- colorQuantile(c("#0000FF80", "#8080FF80", "#FFFFFF80", "#FF808080", "#FF000080"),getisgrid$HOTSPOT, 
  na.color = "transparent")

leaflet(getisgrid) %>% 
  addTiles() %>%
  addPolygons(fillColor =  ~pal(HOTSPOT), fillOpacity = 0.8, weight = 0) %>% 
  addLegend(pal = pal, values = ~HOTSPOT, opacity = 0.7, 
            title = 'BTP crimes')

```


