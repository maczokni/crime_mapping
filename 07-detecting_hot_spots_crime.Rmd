# Chapter 7: Detecting hot spots of crime and repeats

## Introduction

[[[Bla bla bla hot spots of crime]]]

In the previous chapter we explored ways to visualise the concentration of crime incidents in particular location. We used tesselation and kernel density estimation as a way to do this. As noted by @Chainey_2020:

>"Hot spot maps generated using KDE are useful for showing where crime concentrates but may fail to determine unambiguosly what is hot (in hot spot analysis terms) from what is not hot. That is, a KDE hot spot map may fail to separate the significant spatial concentrations of crime from those spatial distributions of less interest, or from random variation"

There are a number of tools that have been developed to detect local clusters. In crime analysis is common the use of the local $Gi^*$ statistic. Whereas in spatial epidemiology a number of techniques have been developed for detecting areas with an unusually high risk of disease. Although originally developed in the context of studying disease, the techniques can also be applied to the study of crime. Most of the methods in spatial epidemiology are based on the idea of "moving windows", such as the spatial scan statistic developed by @Kulldorff_1997. Several packages bring this scan statistics to R. There are also other packages that use different algorithms to detect disease clusters, such as AMOEBA, that is based in the Getis-Ord algorithm. As we will see we are spoiled for choice in this regard.

In crime analysis is also relevant the study of near repeat victimisation. bla bla bla...

For this section we will need the following packages:

```{r}

library(dplyr)

# Packages for handling spatial data and for geospatial carpentry
library(sf)
library(raster)
library(spdep)

# Packages for detecting clusters
library(DCluster)
library(SpatialEpi)
library(AMOEBA)
library(smerc)
library(surveillance)
library(DClusterm)

# Packages for detecting near repeat victimisation
## You will need to install the following package from github
## You can use the following code for it: 
## remotes::install_github("wsteenbeek/NearRepeat", build_vignettes = TRUE)
library(NearRepeat)

# Packages for visualisation and mapping
library(tmap)
library(leaflet)
library(mapview)
library(RColorBrewer)
```

## Getting the data: break and enter in Toronto

For this chapter we travel to Toronto. We will be looking at crime data provided by their Police Service through their Public Safety Data Portal (https://data.torontopolice.on.ca/). We will explore residential break and enter offences for 2019. Beware there is also geomasking going on here. The open data does not include the exact coordinates for these events. For privacy reasons they have been geocoded to the next street intersection. When aggregating the incidents at the neighbourhoud level, as we will do later in this chapter, this may be less problematic. But if you were to be interested in the spatial point pattern level of resolution, you would be coming across artificially created "hot spots" in those street intersections. That´s a problem with most of these open data on crime, but for illustration purposes of the techniques it is ok to use this data - as long as we remember these caveats. 

The cartographic boundary for the city comes from the census office of Canada. We will also need the neighbourhood boundaries for Toronto, which we obtain from the Toronto open data portal (https://open.toronto.ca/). These data is free to redistribute, so for convenience we have assembled it and can be read from our GitHub repository with the following code:

```{r, message = FALSE, warning = FALSE, eval=FALSE}
toronto <- st_read("https://raw.githubusercontent.com/maczokni/crime_mapping/master/data/toronto_c.geojson")
break_enter <- st_read("https://raw.githubusercontent.com/maczokni/crime_mapping/master/data/break_enter.geojson") %>%
  filter((premises_t == "Apartment" | premises_t == "Commercial") & reportedye == "2019")
neighbourhoods <- st_read("https://raw.githubusercontent.com/maczokni/crime_mapping/master/data/nh_profiles_short.geojson")
  
```

```{r, message = FALSE, warning = FALSE, echo=FALSE}
toronto <- st_read("https://raw.githubusercontent.com/maczokni/crime_mapping/master/data/toronto_c.geojson")
break_enter <- st_read("https://raw.githubusercontent.com/maczokni/crime_mapping/master/data/break_enter.geojson") %>%
  filter((premises_t == "Apartment" | premises_t == "Commercial") & reportedye == "2019")
neighbourhoods <- st_read("https://raw.githubusercontent.com/maczokni/crime_mapping/master/data/nh_profiles_short.geojson")
  
```

We can now plot it:

```{r}

tm_shape(toronto) +
  tm_polygons(alpha = 0.3) +
  tm_shape(neighbourhoods) +
  tm_polygons(alpha = 0.3) +
  tm_shape(break_enter) +
  tm_dots(size = 0.01) +
   tm_layout(main.title = "Break and enter in Toronto", 
            main.title.size = 1 ,
            legend.outside = TRUE,  # Takes the legend outside the main map 
            legend.title.size = 0.8)

```

As you can see, and it is often the case, the data requires some cleaning. Some points lie outside the city boundaries, for this we will use code introduced in Chapter 2, and that´s not the only issue.  We also want to make sure there are no records with the same id, there are some in this dataset. So we will filter the data on the unique ID. For filtering unique rows we can rely on the magic of `duplicated()`. This function gives distinct records for which the values in the selected variable are the same. 

```{r}

break_enter <- subset(break_enter, !duplicated(break_enter$event_uniq))
# intersection
break_enter_intersect <- st_intersects(toronto, break_enter)
# subsetting
break_enter <- break_enter[unlist(break_enter_intersect ),]

```

## Micro Grid Cells

In chapter 4 we discussed alternatives to our choropleth maps, and one of these was to use a grid map. This is a process whereby a tessalating grid of a shape such as squares or hexagons is overlaid on top of our area of interest, and our points are aggregated to such a grid. We explored this using `ggplot2` package. We also used this approach when discussing quadrat counting with `ppp` objects and using the functionality provided by the `spatstat` package.

Here we can revisit this with another approach, using the `raster` package. First we need to create the grid for the raster using the `raster()` function. The first argument defines the extent for the grid. By using our "toronto" `sf` object we ensure it includes all of it. Then the `resolution` argument simply defines how large we want the cells in this grid to be. You can play around with this argument and see how the results will vary. Then we will use the `rasterize()` function to count the number of points that fall within each cell. This is just a form of quadrant counting as the one we introduced in Chapter 6. The `rasterize()` function takes as the first argument the coordinates of the points. The second argument provides the grid we just created. The `field` argument in this case is assigned a value of `1`  so that each point is counted a unit. Then we use the `fun` argument to specify we want to `"count"` the points within each cell. The `rasterize()` function by default will set at NA all the values where there are no points. We don´t want this to be the case. So that the raster include those cells as zero we use the `background` argument and set it to zero: 

```{r}
# Create empty raster grid
toronto_r <- raster(toronto, resolution = 0.004)
# Fill the grid with the count of incidents within each cell
break_enter_raster <- rasterize(break_enter,
                                toronto_r,
                                field = 1,
                                fun = "count",
                                background = 0)
# Use base R functionality to plot the raster
plot(break_enter_raster)
plot(st_geometry(toronto), border='#00000040', add=T)
```

This kind of map is helpful if you want to get a sense of the distribution over the whole city, but oftenwise from an operational point of view the interest is at a larger scale of resolution. We want to deep in and observe the micro-places that may be attracting crime within certain parts of the city. We can do this by focusing on particular neighbourhoods.

Let´s find out the neighbourhood with most events. First, we use `st_intersects()` to count points within polygons and then add this to the "neighbourhoods" `sf` object. Then we use `slice_max()` to select the rows with the highest values. Setting the argument `n` to 3, we get the three neighbourhoods with more incidents. For sparser reporting we previously `select()` only the variable containing the neighbourhood name and the count of break and enter occurrences. Using `as.data.frame()` ensures that we don´t retain the geometry and additional elements of the `sf` object when autoprinting the results.

```{r, message = FALSE}
# add point count to each polygon
neighbourhoods$b_e_count <- lengths(st_intersects(neighbourhoods, break_enter))
# identify the neighbourhood with most incidents
as.data.frame(neighbourhoods) %>% 
  dplyr::select(Neighbourh, b_e_count) %>%
  slice_max(b_e_count, n = 3) 
```

Let´s look at Kensington-Chinatown, a well known and distinctive multicultural part of Toronto which is a noted tourist attraction.

```{r, warning = FALSE, message = FALSE}
# Create sf vector with just Kensington boundaries
kensington <- filter(neighbourhoods, 
                     Neighbourh == "Kensington-Chinatown")
# Subset the break and enter incidents within this area of Toronto
break_enter_intersect <- st_intersects(kensington, break_enter)
break_enter_k <- break_enter[unlist(break_enter_intersect ),]

# Create empty grid for Kensington
kensington_r <- raster(kensington, resolution = 0.0009)
# Produce raster object with the number of incidents within each cell
break_enter_raster_k <- rasterize(break_enter_k,
                                kensington_r,
                                field = 1,
                                fun = "count",
                                background = 0)
# Plot results with base R
plot(break_enter_raster_k)
plot(st_geometry(kensington), border='#00000040', add=T)
```
Let´s add some context. We will use `mapview` for it. This is another great package if you want to give your audience some interactivity. It does what is says in the tin: it "provides functions to very quickly and conveniently create interactive visualisations of spatial data. Its main goal is to fill the gap of quick (not presentation grade) interactive plotting to examine and visually investigate both aspects of spatial data, the geometries and their attributes. It can also be considered a data-driven API for the leaflet package as it will automatically render correct map types, depending on the type of the data (points, lines, polygons, raster)." With very few lines of code you can get an interactive plot. Here we first will use `RColorBrewer::brewer.pal()` to define a convenient palette and then use `mapview::mapview()` to plot our raster. Below you see how we increase the transparency with the `alpha.region` argument and how we use `col.regions` to change from the default palette to the one we created.

```{r, message=FALSE, warning=FALSE}

my.palette <- brewer.pal(n = 9, name = "OrRd")
mapview(break_enter_raster_k, 
        alpha.regions = 0.5,
        col.regions = my.palette)
```


## Getis-Ord GI*

The maps above and the ones we introduced in chapter 6 produced through kernel density estimation are helpful for visualising the varying intensity of crime across the study region of interest. As we noted in Chapter 6 it may be hard to discern random variation from clustering of crime incidents simply using these techniques. There are a number of statistical tools that have been developed to extract the signal from the noise, to determine what is hot from random variation.

In crime analysis is common to use a technique developed by Arthur Getis and JK Ord in the early 1990s (@Getis_1992 and @Ord_1995), partly because it was implemented in the widely successful ArcGIS as part of their hot spot analysis tool. Getis and Ord developed what is referred to as the Getis-Ord GI* statistic. This statistic provides a measure of spatial dependence at the local level. They are not testing homogeneity, they are testing dependence of a given attribute around neighbouring areas (which could be operationalised as micro cells in a grid, as above, or could be some form of administrative areas). 

>"This statistic is based on the observation that the distribution of differences in crime intensity between neighbouring areas across a map will follow a normal curve. Therefore, the amount of difference between an area and its neighbors can then be converted to a z-score reflecting the number of standard deviations ($σ$) that the crime level in an area differs from neighborhood normal. Areas with high z-scores (indicating a crime significantly above the mean) are identified as hot spots, while areas with low z-scores are identified as cold spots." @Minn_2020

The first step in computing this statistics involved defining the neighbours of each area. And to do that first we have to turn each cell in our grid into a polygon, so we basically move from the raster to the vector model by virtue of using `raster::rasterToPolygons`. What is a neighbour and the code related to defining neighbours is a topic we explain in much greater detail in chapter 10 when we discuss more generally related measures of dependence. For now, just take the code at face value and understand we are simply looking at each cell and consider that immediately adjacent cells are its neighbours (that´s what the default argument in `spdep::poly2nd()` is doing). What `nb2listw()` does is something we will return to in Chapter 10.

```{r}

# Move to vector model

getisgrid <-  rasterToPolygons(break_enter_raster_k)

# Create the list of neighbors

neighbors <-  poly2nb(getisgrid)
weighted_neighbors <- nb2listw(neighbors, zero.policy=T)


```

Now that we have the data ready we can compute the statistic using the `localG()` function. We can then add the computed local $Gi^*$ to each cell.

```{r, message=FALSE, warning=FALSe}

# Perform the local G analysis (Getis-Ord GI*)

getisgrid$HOTSPOT <- as.vector(localG(getisgrid$layer, weighted_neighbors))


```

We can then see the produced z scores:

```{r}
summary(getisgrid$HOTSPOT)

```

How do you interpret these? They are z scores. So you expect large absolute values to index the existence of cold spots (if they are negative) or hot spots (if they are positive). How large is large? The critical values for the 95th percentile under the assumptions discussed by @Ord_1995 are for $n$=30: 2.93, $n$=50: 3.08, $n$=100: 3.29, $n$=500: 3.71, and for $n$=1000: 3.89. Our sample size is 322 cells.  

```{r}

# Color the grid cells based on the z-score

breaks <- quantile(getisgrid$HOTSPOT, na.rm = TRUE)
palette <- c("#0000FF80", "#8080FF80", "#FFFFFF80", "#FF808080", "#FF000080")
col <- palette[cut(getisgrid$HOTSPOT, breaks)]

# Plot

plot(getisgrid, col=col, border=NA)
plot(st_geometry(kensington), border="gray", add=T)
```

Or on a leaflet map to aid interpretation: 

```{r}

pal <- colorQuantile(c("#0000FF80", "#8080FF80", "#FFFFFF80", "#FF808080", "#FF000080"),getisgrid$HOTSPOT, 
  na.color = "transparent")

leaflet(getisgrid) %>% 
  addTiles() %>%
  addPolygons(fillColor =  ~pal(HOTSPOT), fillOpacity = 0.8, weight = 0) %>% 
  addLegend(pal = pal, values = ~HOTSPOT, opacity = 0.7, 
            title = 'Break and enter')

```


