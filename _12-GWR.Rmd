# Chapter 12: Spatial heterogeneity and regression

## Introduction

One of the challenges of spatial data is that we may encounter exceptions to stationary processes. There may be, for example, parameter variability across the study area. When we have homogeneity everything is the same everywhere, in terms of our regression equation, the parameters are constant. But as we illustrated via an interaction term in the last chapter, that may not be the case. We can encounter situations where a particular input has a different effect in parts of our study area. In this chapter we explore this issue and a number of solutions that have been proposed to deal with it. It goes without saying that having extreme heterogeneity creates technical problems for estimation. If everything is different everywhere we may have to estimate more parameters that we have data for.

One way of dealing with this is by imposing some form of structure, which we did last week, for example when we partition the data and fitted a separate model for the Southern counties. Unlike data partition, **full spatial regimes** also imply using different coefficients for each subset of the data but you fit everything in one go, essentially is the same as running as many models as subsets you have. As Luc Anselin highlights in his lectures, this corrects for heterogeneity but does not explain it. We can then test whether this was necessary, using a *Chow test* comparing the simpler model with the one where we allow variability. Of course, we can also include spatial dependence in these models.

Another way of dealing with heterogeneity is by allowing continuous variation, rather than discrete (as when we subset the data), of the parameters. A popular method for doing this is **geographically weighted regression (GWR)**. This is a case of a local regression where you try to estimate a different set of parameters for each location and these parameters are obtained from a subset of observation using kernel regression. In traditional local regression we select the subset in the attribute space (observations with similar attributes), in GWR the local subset is defined in a geographical sense using a kernel (remember how we used kernels for density estimation to define a set of "neighbors"). We are estimating regression equations based on nearby locations as specified by our kernel and this produces a different coefficient for each location. As with kernel density estimation you can distinguish between fixed bandwidth and adaptive bandwidth.

For this session we will need to load the following packages:

```{r libraryload, message=FALSE, warning=FALSE}

library(tidyr)
library(dplyr)
# Packages for handling spatial data and for geospatial carpentry
library(sf)
library(sp)
library(spdep)
# Packages for regression and spatial regression
library(spatialreg)
# Packages for mapping and visualisation
library(tmap)
library(ggplot2)
# Packages with spatial datasets
library(geodaData)
```


## Spatial regimes

### Fitting the constrained and the unconstrained models

To illustrate this solution we will go back to the "ncovr" data we explored last week and use the same spatial weight matrix as in the previous chapter. If you have read the -@Baller_2001 paper that we are in a way replicating here, you could see they decided that they needed to run separate models for the South and the North. LetÂ´s explore this here.

```{r}
data("ncovr")
# We coerce the sf object into a new sp object
ncovr_sp <- as(ncovr, "Spatial")
# Then we create a list of neighbours using 10 nearest neighbours criteria
xy <- coordinates(ncovr_sp)
nb_k10 <- knn2nb(knearneigh(xy, k = 10))
rwm <- nb2mat(nb_k10, style='W')
rwm <- mat2listw(rwm, style='W')
# remove redundant objects
rm(list = c("ncovr_sp", "xy", "nb_k10"))
```

We can start by running a model for the whole country to use as a baseline model:

```{r}
fit1_70 <- lm(HR70 ~ RD70 + PS70 + MA70 + DV70 + UE70, data = ncovr)
summary(fit1_70)
```

A simple visualisation of the residuals suggest the model may not be optimal for the entire study region:

```{r}
ncovr$res_fit1 <- fit1_70$residuals
ggplot(ncovr, aes(x = res_fit1, colour = as.factor(SOUTH))) + 
  geom_density() 
```

To be able to fit these spatial regimes, that will allow for separate coefficients for counties in the North and those in the South, first we need to define a dummy for the Northern counties, and we create a vector of 1.

```{r}
ncovr$NORTH <- recode(ncovr$SOUTH, `1` = 0, `0` = 1)
ncovr$ONE <- 1
```

Having created these new columns we can fit a standard OLS model interacting the inputs with our dummies and setting the intercept to 0, so that each regime is allowed its own intercept:

```{r}
fit2_70_sr <- lm(HR70 ~ 0 + (ONE + RD70 + PS70 + MA70 + DV70 + UE70):(SOUTH + NORTH), data = ncovr)
summary(fit2_70_sr)
```

We can see a higher intercept for the South, reflecting the higher level of homicide in these counties. Aside from this, the two more notable differences are, on the one hand, the insignificant effect of unemployment in the North, and the higher impact of the divorce rate in the North.

### Running the Chow test

The Chow Test is often used in econometrics to determine whether the independent variables have different impacts on different subgroups of the population. -@Anselin_2007 provides an implementation for this test, that extracts the residuals and the degrees of freedom from the constrained (the simpler oLS model) and the unconstrained models (the less parsimonious spatial regime), and then computes the test based on the F distribution.

```{r}
chow.test <- function(rest,unrest)
{
er <- residuals(rest)
eu <- residuals(unrest)
er2 <- sum(er^2)
eu2 <- sum(eu^2)
k <- rest$rank
n2k <- rest$df.residual - k
c <- ((er2 - eu2)/k) / (eu2 / n2k)
pc <- pf(c,k,n2k,lower.tail=FALSE)
list(c,pc,k,n2k)
}

```

With the function in our environment we can then run the test, which will yield the test statistic, the p value, and two degrees of freedom.

```{r}
 chow.test(fit1_70,fit2_70_sr)

```

How we interpret this test? When the Chow test is significant, as here, this provides evidence of spatial heterogeneity and in favour of fitting a spatial regime model, allowing for non-constant coefficients across the discrete subsets of data that we have specified.

### Spatial dependence with spatial heterogeneity

So far we have only adjusted for a form of spatial heterogeneity in our model, but we may still have to deal with spatial dependence. Here what we covered in the previous chapter, still applies. We can run the various specification tests and then select a model that adjust for spatial dependence.

```{r}
summary(lm.LMtests(fit2_70_sr, rwm, test = c("LMerr","LMlag","RLMerr","RLMlag")))
```

The Lagrange multiplier tests, according to Anselin suggestions for how to interpret these, point to a spatial lag model (since RLMlag is several orders more significant than the error alternative).

We could then fit this model using `sp

```{r}
fit3_70_sr_sar <- lagsarlm(HR70 ~ 0 + (ONE + RD70 + PS70 + MA70 + DV70 + UE70):(SOUTH + NORTH), data = ncovr, rwm)
summary(fit3_70_sr_sar)
```

We see that the AIC is better than for the non-spatial model and that there is no more residual autocorrelation. We could explore other specifications (ie spatial Durbin model) and the same caveats about interpreting coefficients in a spatial lag model we raised in the previous chapter would apply here. If the Lagrange multiplier tests had suggested a spatial error model, we could also have fitted the model in the way we have also already covered simply adjusting the regression formula as in this section.

We can now also run a Chow test, only in this case, we need to adjust the previous one to account for spatial dependence. -@Anselin_2007 also providesfor an implementation of this test in R with the following ad hoc function:

```{r}
spatialchow.test <- function(rest,unrest)
{
lrest <- rest$LL
lunrest <- unrest$LL
k <- rest$parameters - 2
spchow <- - 2.0 * (lrest - lunrest)
pchow <- pchisq(spchow,k,lower.tail=FALSE)
list(spchow,pchow,k)
}
```

This function will test a spatial lag model for the whole country as a whole with the spatial lag model with our data partitions. So first we need to estimate the appropriate lag model to be our constrained model:

```{r}
fit4_70_sar <- lagsarlm(HR70 ~ RD70 + PS70 + MA70 + DV70 + UE70, data = ncovr,
                    rwm)

```

Which we can then use in the newly created function:

```{r}
spatialchow.test(fit4_70_sar, fit3_70_sr_sar)
```

The test is again highly significant providing evidence for the spatial lag model with the spatial regimes.

## GWR

The basic idea behind GWR is to explore how the relationship between a dependent variable ($Y$) and one or more independent variables (the $X$s) might vary geographically. Instead of assuming that a single model can be fitted to the entire study region, it looks for geographical differences. GWR evaluates a local model of the variable or process you are trying to understand or predict by fitting a regression equation to every feature in the dataset. GWR constructs these separate equations by incorporating the dependent and explanatory variables of the features falling within the neighborhood of each target feature. 

Geographically Weighted Regression can be used for a variety of applications, including the following:

- Is the relationship between educational attainment and income consistent across the study area?
- Do certain illness or disease occurrences increase with proximity to water features?
- What are the key variables that explain high forest fire frequency?
- Which habitats should be protected to encourage the reintroduction of an endangered species?
- Where are the districts in which children are achieving high test scores? What characteristics seem to be associated? Where is each characteristic most important?
- Are the factors influencing higher cancer rates consistent across the study area?

GWR provides three types of regression models: Continuous, Binary, and Count. These types of regression are known in statistical literature as Gaussian, Logistic, and Poisson, respectively. The Model Type for your analysis should be chosen based on how your Dependent Variable was measured or summarized as well as the range of values it contains. 

GWR works by moving a search window (defined by our kernel) from one point in a data set to the next, working through them all in sequence. As the search window rests on asamplepoint, all other points that are around it and within the search window are identified. A regression model is then fitted to that subset of the data, giving most weight to the points that are closest to the one at the centre. For a data set of 2536 observations GWRwill, then, fit 2536 weighted regression models, the results of which are compared to look for geographical variation.This immediately raises a question âwhatarea should the search window cover each time? The answer is provided by a process of calibration, to select an âoptimalâ bandwidth (an optimal search window size).Note that the distance from one point to another can be defined in two ways: either by actual geographic distance or by whether itâs the first nearest neighbour, the second, the third and so forth. If the number of neighbours within the search window is fixed then it willvaryin areafrom point to point: where the sample points are close together thewindow will have less area;where the points are sparse it will fill a greater area. This is called an adaptive window and is usually better for analysing census data (because census zones are of a variable size: smaller where population density ishigher and vice versa). 


```{r getdataforgwr}



toronto <- st_read("data/toronto_c.geojson")
# crimes <- st_read("data/neighbourhood-crime-rates.geojson")
ksi_collisions <- st_read("data/Motor Vehicle Collisions with KSI Data.geojson")
wellbeing <- readxl::read_xlsx("data/wellbeing-toronto-environment.xlsx")



num_ksi <- ksi_collisions %>% 
  filter(YEAR == 2019) %>% 
  st_drop_geometry() %>% 
  mutate(ksi = ifelse(INJURY %in% c("Fatal","Major"), 1, 0), 
         speed = ifelse(SPEEDING == "Yes", 1, 0), 
         agg_driv = ifelse(AG_DRIV == "Yes", 1, 0), 
         red_light = ifelse(REDLIGHT == "Yes", 1, 0), 
         alcohol = ifelse(ALCOHOL == "Yes", 1, 0)) %>% 
  group_by(ACCNUM, LIGHT, VISIBILITY, RDSFCOND) %>% 
  # summarise(prop_ksi = mean(ksi))
  summarise(prop_ksi = sum(ksi),
            speed = ifelse(sum(speed, na.rm = T)> 0, 1, 0),
            agg_driv = ifelse(sum(agg_driv, na.rm = T)> 0, 1, 0),
            red_light = ifelse(sum(red_light, na.rm = T)> 0, 1, 0),
            alcohol = ifelse(sum(alcohol, na.rm = T)> 0, 1, 0), 
         visibility = ifelse(VISIBILITY == "Clear", 0, 1), 
         daylight = ifelse(LIGHT == "Daylight", 0, 1), 
         rd_dry = ifelse(RDSFCOND == "Dry", 0, 1)
         ) 

num_ksi <- num_ksi %>% unique()
  
ggplot(num_ksi, aes(x = prop_ksi)) + 
  geom_histogram()



fit.ols<-glm(prop_ksi ~ speed + agg_driv + red_light + alcohol, data = num_ksi) 
summary(fit.ols)

num_ksi$resids<-residuals(fit.ols)
num_ksi$res_quantiles = cut(num_ksi$resids, breaks = c(quantile(num_ksi$resids)[1:3],quantile(num_ksi$resids)[4]+0.00000001,quantile(num_ksi$resids)[5]) , include.lowest = TRUE, dig.lab=10)
colours <- c("dark blue", "blue", "red", "dark red")

ggplot(num_ksi, aes(x = resids)) + 
  geom_histogram()


num_ksi <- left_join(ksi_collisions %>% filter(YEAR == "2019") %>% select(ACCNUM) %>% unique(), num_ksi, by = c("ACCNUM" = "ACCNUM"))

ggplot() + 
  ggspatial::annotation_map_tile() + 
  geom_sf(data = num_ksi, aes(col = resids), alpha = .9) + 
  scale_color_gradient2()

```




```{r}

thing <- ksi_collisions %>% 
  filter(YEAR == 2018) %>% 
  st_drop_geometry() %>% 
  group_by(INVTYPE, INJURY) %>% count()

length(unique(ksi_collisions$ACCNUM))



thing <- ksi_collisions %>% 
  st_drop_geometry() %>% 
  group_by(ACCNUM, INVTYPE) %>% 
  count() %>% 
  pivot_wider(id_cols = ACCNUM, names_from = INVTYPE, values_from = n) %>% 
  replace_na()


df <- tibble(x = c(1, 2, NA), y = c("a", NA, "b"))


```



```{r}

accs_2019 <- ksi_collisions %>% 
  filter(YEAR == 2019) %>% 
  st_drop_geometry() 

  group_by(ACCNUM, LIGHT, VISIBILITY, RDSFCOND) %>% 
  # summarise(prop_ksi = mean(ksi))
  summarise(prop_ksi = sum(ksi),
            speed = ifelse(sum(speed, na.rm = T)> 0, 1, 0),
            agg_driv = ifelse(sum(agg_driv, na.rm = T)> 0, 1, 0),
            red_light = ifelse(sum(red_light, na.rm = T)> 0, 1, 0),
            alcohol = ifelse(sum(alcohol, na.rm = T)> 0, 1, 0), 
         visibility = ifelse(VISIBILITY == "Clear", 0, 1), 
         daylight = ifelse(LIGHT == "Daylight", 0, 1), 
         rd_dry = ifelse(RDSFCOND == "Dry", 0, 1)
         ) 

num_ksi <- num_ksi %>% unique()

```

GWR is a good **exploratory** tool but there are some issues. 
Practical issues with GWR (see Anselin lecturem 1.03):
- choice of bandwidth
- parameter inference



## Recommended reading

Luc Anselin lecture on specification of spatial heterogeneity is particularly helpful as an introduction to the issues and solutions we have discussed in this chapter.

Fotheringham et al.(2002) Geographically Weighted Regression: The Analysis of Spatially Varying Relationship, published by Wiley

Spatial random effects... (CAR and Bayesian models)
