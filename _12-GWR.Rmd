# Chapter 12: Spatial heterogeneity and regression

## Introduction

One of the challenges of spatial data is that we may encounter exceptions to stationary processes. There may be, for example, parameter variability across the study area. When we have homogeneity everything is the same everywhere, in terms of our regression equation, the parameters are constant. But as we illustrated via an interaction term in the last chapter, that may not be the case. We can encounter situations where a particular input has a different effect in parts of our study area. In this chapter we explore this issue and a number of solutions that have been proposed to deal with it. It goes without saying that having extreme heterogeneity creates technical problems for estimation. If everything is different everywhere we may have to estimate more parameters that we have data for.

One way of dealing with this is by imposing some form of structure, which we did last week, for example when we partition the data and fitted a separate model for the Southern counties. Unlike data partition, **full spatial regimes** also imply using different coefficients for each subset of the data but you fit everything in one go, but essentially is the same as running as many models as subsets you have. As Luc Anselin highlights in his lectures, this corrects for heterogeneity but does not explain it. We can then test whether this was necessary, using a *Chow test* comparing the simpler model with the one where we allow variability. Of course, we can also include spatial dependence in these models.



[[Already have an introduction to spatial hetereogeneity in the intro to the next chapter, where we say we will talk about solutions here. It possibly makes sense to move here the business about spatial regimes and data partition as a first way to deal with spatial heterogeneity and then introduce GWR as a more localised solution]]

## Heterogeneity, spatial regimes, and data partition

Before  we  proceed  to  a  more  detailed  description  of  these  two models  it is  important  that  we return to the issue of heterogeneity. We looked at whether the role of how resource deprivation was different in Southern and Northern states. We found that this interaction was indeed significant. Resource deprivation had a more significant effect in Southern than in Northern states. 
 
If you have read the Baller et al. (2001) paper that we are in a way replicating here, you could see they decided that they needed to run separate models for the South and the North....  

This kind of  situation,  where  sub-regions, seem  to  display  different  patterns often  is alluded with  the  name  of  spatial  regimes.  In  the  context of  regression  analysis,  spatial  regimes relates to the possibility that we may need to split our data into two (or more sub-regions) in order to run our models, because we presume that the relationship of the predictors to the outcome may play out differently in these sub-regions (spatial regimes). 
 
So  how  can  we  assess  whether  this  is  an  issue  in  our  data?  As with  many  other  diagnostics  of regression,  you  may  want  to  start  by  looking  at  your  residuals.  Look  at  the  residual  map  we produced earlier. Do you think that the residuals look different in the South and in the North? If the pattern is not clear to you, you may want to run other forms of visualisation. 
 
```{r}

ggplot(ncovr_sf, aes(x = res_fit1, colour = as.factor(SOUTH))) + 
  geom_density() 
```

What do you see in this plot? And, critically, what does it mean? What is this telling you about the predicted values that result from our model? (Remember what a residual is: the difference between the observed values and the predicted values). 

There are formal tests that one can use to further explore these issues. The paper by Bollen et al. (2001) mentions them (Chow tests). But those are beyond the scope of this course. Sufficient to say that,  as  Bollen  et  al.  (2001),  we  are  going  to  split  our  analysis  and  run  them  separately  for  the Southern and the Northern states. We have covered the `filter()` function from `dplyr` to split datasets based on values of a variable. But to split `sf` objects it is better to rely on the more generic subset function, since `filter()` doesn't accommodate well the column with the geographic information that `sf` provides.

```{r}
ncovr_s_sf <- subset(ncovr_sf, SOUTH == 1)
ncovr_n_sf <- subset(ncovr_sf, SOUTH == 0)
```
 `DV90` divorce rate, `MA90` the median age, `PS90` - population structure, and `UE90` unemployment. 

## GWR

The basic idea behind GWR is to explore how the relationship between a dependent variable ($Y$) and one or more independent variables (the $X$s) might vary geographically. Instead of assuming that a single model can be fitted to the entire study region, it looks for geographical differences. GWR evaluates a local model of the variable or process you are trying to understand or predict by fitting a regression equation to every feature in the dataset. GWR constructs these separate equations by incorporating the dependent and explanatory variables of the features falling within the neighborhood of each target feature. 

Geographically Weighted Regression can be used for a variety of applications, including the following:

- Is the relationship between educational attainment and income consistent across the study area?
- Do certain illness or disease occurrences increase with proximity to water features?
- What are the key variables that explain high forest fire frequency?
- Which habitats should be protected to encourage the reintroduction of an endangered species?
- Where are the districts in which children are achieving high test scores? What characteristics seem to be associated? Where is each characteristic most important?
- Are the factors influencing higher cancer rates consistent across the study area?

GWR provides three types of regression models: Continuous, Binary, and Count. These types of regression are known in statistical literature as Gaussian, Logistic, and Poisson, respectively. The Model Type for your analysis should be chosen based on how your Dependent Variable was measured or summarized as well as the range of values it contains. 

GWR works by moving a search window from one point in a data set to the next, working through them all in sequence. As the search window rests on asamplepoint, all other points that are around it and within the search window are identified. A regression model is then fitted to that subset of the data, giving most weight to the points that are closest to the one at the centre. For a data set of 2536 observations GWRwill, then, fit 2536 weighted regression models, the results of which are compared to look for geographical variation.This immediately raises a question –whatarea should the search window cover each time? The answer is provided by a process of calibration, to select an “optimal” bandwidth (an optimal search window size).Note that the distance from one point to another can be defined in two ways: either by actual geographic distance or by whether it’s the first nearest neighbour, the second, the third and so forth. If the number of neighbours within the search window is fixed then it willvaryin areafrom point to point: where the sample points are close together thewindow will have less area;where the points are sparse it will fill a greater area. This is called an adaptive window and is usually better for analysing census data (because census zones are of a variable size: smaller where population density ishigher and vice versa). 


```{r getdataforgwr}

library(sf)

toronto <- st_read("data/toronto_c.geojson")
# crimes <- st_read("data/neighbourhood-crime-rates.geojson")
ksi_collisions <- st_read("data/Motor Vehicle Collisions with KSI Data.geojson")
wellbeing <- readxl::read_xlsx("data/wellbeing-toronto-environment.xlsx")

library(ggplot2)
library(dplyr)

num_ksi <- ksi_collisions %>% 
  filter(YEAR == 2019) %>% 
  st_drop_geometry() %>% 
  mutate(ksi = ifelse(INJURY %in% c("Fatal","Major"), 1, 0), 
         speed = ifelse(SPEEDING == "Yes", 1, 0), 
         agg_driv = ifelse(AG_DRIV == "Yes", 1, 0), 
         red_light = ifelse(REDLIGHT == "Yes", 1, 0), 
         alcohol = ifelse(ALCOHOL == "Yes", 1, 0)) %>% 
  group_by(ACCNUM, LIGHT, VISIBILITY, RDSFCOND) %>% 
  # summarise(prop_ksi = mean(ksi))
  summarise(prop_ksi = sum(ksi),
            speed = ifelse(sum(speed, na.rm = T)> 0, 1, 0),
            agg_driv = ifelse(sum(agg_driv, na.rm = T)> 0, 1, 0),
            red_light = ifelse(sum(red_light, na.rm = T)> 0, 1, 0),
            alcohol = ifelse(sum(alcohol, na.rm = T)> 0, 1, 0), 
         visibility = ifelse(VISIBILITY == "Clear", 0, 1), 
         daylight = ifelse(LIGHT == "Daylight", 0, 1), 
         rd_dry = ifelse(RDSFCOND == "Dry", 0, 1)
         ) 

num_ksi <- num_ksi %>% unique()
  
ggplot(num_ksi, aes(x = prop_ksi)) + 
  geom_histogram()



fit.ols<-glm(prop_ksi ~ speed + agg_driv + red_light + alcohol, data = num_ksi) 
summary(fit.ols)

num_ksi$resids<-residuals(fit.ols)
num_ksi$res_quantiles = cut(num_ksi$resids, breaks = c(quantile(num_ksi$resids)[1:3],quantile(num_ksi$resids)[4]+0.00000001,quantile(num_ksi$resids)[5]) , include.lowest = TRUE, dig.lab=10)
colours <- c("dark blue", "blue", "red", "dark red")

ggplot(num_ksi, aes(x = resids)) + 
  geom_histogram()


num_ksi <- left_join(ksi_collisions %>% filter(YEAR == "2019") %>% select(ACCNUM) %>% unique(), num_ksi, by = c("ACCNUM" = "ACCNUM"))

ggplot() + 
  ggspatial::annotation_map_tile() + 
  geom_sf(data = num_ksi, aes(col = resids), alpha = .9) + 
  scale_color_gradient2()

```




```{r}

thing <- ksi_collisions %>% 
  filter(YEAR == 2018) %>% 
  st_drop_geometry() %>% 
  group_by(INVTYPE, INJURY) %>% count()

length(unique(ksi_collisions$ACCNUM))

library(tidyr)

thing <- ksi_collisions %>% 
  st_drop_geometry() %>% 
  group_by(ACCNUM, INVTYPE) %>% 
  count() %>% 
  pivot_wider(id_cols = ACCNUM, names_from = INVTYPE, values_from = n) %>% 
  replace_na()


df <- tibble(x = c(1, 2, NA), y = c("a", NA, "b"))


```



```{r}

accs_2019 <- ksi_collisions %>% 
  filter(YEAR == 2019) %>% 
  st_drop_geometry() 

  group_by(ACCNUM, LIGHT, VISIBILITY, RDSFCOND) %>% 
  # summarise(prop_ksi = mean(ksi))
  summarise(prop_ksi = sum(ksi),
            speed = ifelse(sum(speed, na.rm = T)> 0, 1, 0),
            agg_driv = ifelse(sum(agg_driv, na.rm = T)> 0, 1, 0),
            red_light = ifelse(sum(red_light, na.rm = T)> 0, 1, 0),
            alcohol = ifelse(sum(alcohol, na.rm = T)> 0, 1, 0), 
         visibility = ifelse(VISIBILITY == "Clear", 0, 1), 
         daylight = ifelse(LIGHT == "Daylight", 0, 1), 
         rd_dry = ifelse(RDSFCOND == "Dry", 0, 1)
         ) 

num_ksi <- num_ksi %>% unique()

```



## Recommended reading

Fotheringham et al.(2002) Geographically Weighted Regression: The Analysis of Spatially Varying Relationship, published by Wiley
